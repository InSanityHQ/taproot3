<html><head><meta charset="utf-8" /><meta content="width=device-width, initial-scale=1.0" name="viewport" /><meta content="Taproot Authors" name="author" /><meta content="A collection of academic and project notes" name="description" /><title>Taproot</title><link href="/static/css/firn_base.css" rel="stylesheet" /><link href="/static/css/prism.css" rel="stylesheet" /><link href="/static/css/global.css" rel="stylesheet" /><script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js" type="text/javascript"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        'displayAlign': 'center',
        'displayIndent': '0em',
        'extensions': ['tex2jax.js'],
        'tex2jax': {
        'inlineMath': [ ['$','$'], ['\\(','\\)'] ],
        'processEscapes': true
        },
  loader: {load: ['[tex]/cases']},
  tex: {packages: {'[+]': ['cases']}},
        'HTML-CSS': { scale: 100,
                        linebreaks: { automatic: 'false' },
                        webFont: 'TeX'
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: 'false' },
              font: 'TeX'},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: 'AMS'},
               MultLineWidth: '85%',
               TagSide: 'right',
               TagIndent: '.8em'
             }})</script></head><body><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-core.min.js" type="text/javascript"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/autoloader/prism-autoloader.min.js" type="text/javascript"></script><div class="headerline"><a class="wordmark" href="https://taproot3.sanity.gq/" style="border:0">TR3</a></div><main><article class="content rss"><div class="preamble"><h1 class="title"></h1></div><div class="metamble"><span><span>Written by </span><span></span></span></div><div class="notebody"><div><section><p><span>title: Bio Engaging with research project context: BIO201 author: Huxley
source: ---</span></p><p><span>#ref #ret</span></p><table><tr><td><span>## Record of understanding:</span></td></tr><tr><td><code>The writing style here is more similar to my notes, which I personally use as a record of my understanding. If you would like this to be written up more formally, please let me know!</code></td></tr><tr><td><span>### 1. </span><strong><span>Paper Title:</span></strong><span> > A COVID-19 pandemic AI-based system with deep learning forecasting and automatic statistical data acquisition: Development and Implementation Study</span></td></tr><tr><td><span>### 2. </span><strong><span>What question or phenomenon was being investigated in this study and why?</span></strong><span> - COVID is a large problem, almost all of the AI studies around it are region specific or centered around a single country - CPAIS (COVID-19 Pandemic AI System) aims to solve this by automatically compiling databases to form a worldwide dateset - Including governmental responses - allows heatmap viz of different policies in different countries, - and predictive modeling, ofc.</span></td></tr><tr><td><span>### 3. </span><strong><span>What background information did you need to understand in order to understand the question, main experiment, main finding, and significance? (i.e.Â what did you find yourself digging into in the introduction or through outside sources; what was the most relevant background info?)</span></strong></td></tr><tr><td><span>##### ARIMA: </span><em><span>Auto Regressive Integrated Moving Average</span></em><span> - Type of Auto Regressive (AR, not Augmented Reality) model - used for processes which change over time (economics, web traffic, ect.) - Very similar to ARMA, but with a more complex stochastic term - Uses time series data - ie. use differences between values rather than the actual value - p, d, q - p: order of AR term, or time lag - d: "degree of differencing" - q: order of MA term, or size of moving average window</span></td></tr><tr><td><span>> </span><strong><span>Predicted Y_t = Constant + Linear combination Lags of Y (upto p lags) + Linear Combination of Lagged forecast errors (upto q lags)</span></strong><span> - </span><a class="firn-external" href="https://www.machinelearningplus.com/time-series/arima-model-time-series-forecasting-python" target="_blank">Good article</a></td></tr><tr><td><span>##### MLP: </span><em><span>Multilayer Perceptron Neural Networks</span></em><span> - Just a feed forward network, but with hidden layers. Pretty ambiguous term, left relatively ambiguous in the paper. - Fancy name for a vanilla neural net - They highlight: designed to solve non-linearly separable problems - with sigmoid - They used </span><a class="firn-external" href="https://github.com/trnnick/nnfor" target="_blank">nnfor</a><span>, which means they used R.</span></td></tr><tr><td><span>##### MAE (vs RMSE): - Mean Absolute Error, which I haven't used before, is like RMSE (root mean square error) but less punishing for bigger errors.</span></td></tr><tr><td><span>##### MAPE: </span><em><span>Mean Absolute Percentage Error</span></em></td></tr><tr><td><span>- aka Mean Absolute Percentage Deviation - loss function, used for measuring forecast accuracy. - average of percentage errors - generally not good..</span></td></tr><tr><td><span>### 4. What was the main thing the researchers found out and how did they do so?...</span></td></tr><tr><td><span>1. what were the main (1-2) experiments? 1. Dateset gen and renewal: 1. using crawler on source 2. integrating the data into existing database 3. statistical analysis with predefined procedure 1. The experimentation comes in at this step with different deep learning models and techniques 2. </span><strong><span>Main Models:</span></strong><span> 1. LSTM: </span><em><span>long term short term memory</span></em><span> 1. A type of recurrent neural network which uses a 'cell state' and forget gates to have both long and short term memory 2. better forecast accuracy than other models 2. ARIMA: </span><em><span>Auto Regressive Integrated Moving Average</span></em><span> 1. > statistical learning model with time series regression</span></td></tr><tr><td><span>2. what data did they generate? 1. Generated a worldwide automatically updating dateset for COVID-19 with their automation technology 2. And also, generated consistent forecasting data of the next 14 days 3. Finally, generated data on the correlation of government policies and COVID-19</span></td></tr><tr><td><span>3. what does that data mean? 1. The dateset, innit of itself, means nothing. 2. the forecasting data means what we can expect for the next two weeks 3. the correlation data </span><strong><em><span>can</span></em></strong><span> mean what governmental policies work best for dealing with COVID-19</span></td></tr><tr><td><span>### 5. What was the significance or larger impact of the main finding?</span></td></tr><tr><td><span>- the dateset itself is useful for the entire world doing data analysis - the forecasting data can help with rapid policy changes and preparations. Being able to see two weeks into the future, especially at the beginning of the pandemic, was and is incredibly useful. - they also made ways to visualize the data, as human understanding is ultimately the most important at this point - they didn't make very substantive claims about governmental policies - instead, they mostly made a very useful tool for data viz, prediction, and comparison. In the actual experimentation with the types of models, as expected, LSTMs did better. - they were experimenting with ARIMA, but ultimately, LSTMs win again.</span></td></tr><tr><td><span>## Reflection Questions</span></td></tr><tr><td><span>1. </span><strong><span>What paper did you choose and why did you choose it?</span></strong><span> 1. > A COVID-19 pandemic AI-based system with deep learning forecasting and automatic statistical data acquisition: Development and Implementation Study 2. I was interested in doing some similar things earlier, so this paper caught my eye.</span></td></tr></table><ol><li><p><span>*How did you go about trying to understand the paper that you chose?
   What was your reading/understanding process like and why did you
   employ that strategy?*</span></p><ol><li><p><span>Generally speaking, my strategy was to get background on the topic
      (normally by reading the abstract) and then follow my curiosity
      throughout the paper. When their was something I didn't know about
      or understand, I would look for more info on that first in the
      paper then in outside resources.</span></p></li><li><p><span>I read the abstract of the paper, then learned about the terms I
      didn't understand from the </span><em><span>Introduction</span></em><span> and </span><em><span>Materials and
      Methods</span></em><span> sections. I also used the handy cmd-f functionality to
      search through the document. However, a lot of my time was spent
      looking at other articles online for deeper explanations. I then
      jumped into the data at the bottom, then read through the
      </span><em><span>Discussion</span></em><span> and such.</span></p></li><li><p><span>This seemed like the best way to go about understanding a paper
      with a topic that I was already somewhat familiar with yet had a
      lot of new terms.</span></p></li></ol><hr /></li><li><p><span>*What did you find challenging about trying to understand your paper?
   Although the task may have felt generally challenging, try to get
   specific here.*</span></p><ol><li><p><span>Specifically to this paper, trying to understand ARIMA was by far
      the most complex part.</span></p></li><li><p><span>The actual format and layout of the paper felt pretty familiar,
      and wasn't hard to navigate.</span></p></li></ol><hr /></li><li><p><strong><span>What do you think you could try next time that might improve your
   process?</span></strong></p><ol><li><p><span>I'm still debating how to order looking at data and discussion /
      conclusions. I spent a while looking at data before the
      discussion, then I went back to the data after I had finished
      reading. I don't want to bias my understanding of the data, but I
      also don't want to waste a lot of time looking at data without
      much background to understand it. Next time, I think I will try
      looking at the data after and seeing if my thoughts are clouded by
      the discussion.</span></p></li></ol></li></ol><hr /><ol><li><p><span>*What type of previous experience do you have with reading papers
   from the scientific literature (either review articles or primary
   research)?*</span></p><ol><li><p><span>I do a lot of projects in my free time, which often lead to me
      having to read papers for info. Just last night, actually, I spent
      a few hours reading scientific papers on different spike detection
      algorithms as well as Kalman filters.</span></p></li></ol></li></ol></section></div></div></article><div class="metapanel"><div class="metalabel">Contents</div><div class="metalabel">Backlinks</div></div></main></body></html>