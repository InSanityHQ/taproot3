<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2021-10-14 Thu 19:38 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Cryptography</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Huxley Marvit" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" href="/global.css" type="text/css"/>
<link rel="stylesheet" href="/admonition.css" type="text/css"/>
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js", "[Contrib]/siunitx/siunitx.js", "[Contrib]/mhchem/mhchem.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        TeX: {extensions: ["AMSmath.js","AMSsymbols.js",  "[Contrib]/siunitx/siunitx.js", "[Contrib]/mhchem/mhchem.js"]},
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="preamble" class="status">
<div class="header"><span class="site"><a href="https://taproot3.sanity.gq">TR3.5</a></span></div><div class="datarow"><h1 class="title">Cryptography</h1> <h2 class="subtitle"></h2> <span class="author">Huxley Marvit</span> <span class="date">2021-10-14 Thu 19:38</date></div>
</div>
<div id="content">
<div id="linktables"><div id="table-of-backlinks"><h2>Backlinks</h2><ul><li><a class='backlink' href='index.html'>CS240 Master Index</a></li> <li><a class='backlink' href='KBxCryptography.html'>Cryptography</a></li> </ul></div><div id="table-of-contents"><h2>Table of Contents</h2><div id="text-table-of-contents">
<ul>
<li><a href="#cryptography">1. Cryptography</a>
<ul>
<li><a href="#creating-a-custom-hash-function">1.1. Creating a custom hash function</a>
<ul>
<li><a href="#requirements">1.1.1. Requirements</a></li>
<li><a href="#nn-hash">1.1.2. NN hash</a></li>
<li><a href="#proof-of-concept">1.1.3. Proof of concept</a></li>
<li><a href="#results">1.1.4. Results</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div></div>
</div>
<p>
#ret #hw
</p>

<hr />

<div id="outline-container-cryptography" class="outline-2">
<h2 id="cryptography"><span class="section-number-2">1</span> Cryptography</h2>
<div class="outline-text-2" id="text-cryptography">
<pre class="example" id="org98a049b">
This is a bit of a weirder one..
</pre>

<p>
Done prior:
</p>

<pre class="example" id="org2f02c57">
-   Try turning on two-factor authentication, if you have that option.
-   Think about how secure your password is, relative to how attackers would try guessing. Is it a dictionary or relatively common word? Is it all lower case letters, or do you also include uppercase letters, numbers, and/or symbols? Think about how large a search space is needed to find your password. Do some
-   Generate a public/private key pair for yourself. Put the public key on our test laptop so that you can SSH into your account without typing a password. You could also use this key for your Github account, if you have one.
</pre>
</div>

<div id="outline-container-creating-a-custom-hash-function" class="outline-3">
<h3 id="creating-a-custom-hash-function"><span class="section-number-3">1.1</span> Creating a custom hash function</h3>
<div class="outline-text-3" id="text-creating-a-custom-hash-function">
</div>
<div id="outline-container-requirements" class="outline-4">
<h4 id="requirements"><span class="section-number-4">1.1.1</span> Requirements</h4>
<div class="outline-text-4" id="text-requirements">
<p>
What are the requirements for a hash function?
</p>

<ul class="org-ul">
<li><a href="https://stackoverflow.com/questions/2889473/when-is-it-safe-to-use-a-broken-hash-function">Source</a>

<ul class="org-ul">
<li>No preimage: given \(y\), it should not be feasible to find \(x\)
such that \(h(x) = y\).</li>
<li>No second preimage: given \(x_1\), it should not be feasible to find
\(x_2\) (distinct from \(x_1\)) such that \(h(x_1) = h(x_2)\).</li>
<li>No collision: it should not be feasible to find any \(x_1\) and
\(x_2\) (distinct from each other) such that \(h(x_1) = h(x_2)\).</li>
</ul></li>
</ul>

<pre class="example" id="org89a2059">
What if we just.. use a neural network?
</pre>
</div>
</div>

<div id="outline-container-nn-hash" class="outline-4">
<h4 id="nn-hash"><span class="section-number-4">1.1.2</span> NN hash</h4>
<div class="outline-text-4" id="text-nn-hash">
<p>
Intuitively, using a neural network as a hash function seems like it
won't work. In fact, I believe it doesn't work, <b>I just don't know why
it doesn't work.</b> <b>*</b> Of course, a vanilla neural network won't work
because we can just train a model to reverse its mapping. To solve this
problem, we can use something I call permute layers.
</p>
</div>

<ol class="org-ol">
<li><a id="permute-layers"></a>Permute Layers<br />
<div class="outline-text-5" id="text-permute-layers">
<p>
<b>Concept --</b> Fundamentally, the concept of permute layers is to take an
input, and do some operation on it such that a non-continuous output
space is generated. The goal would be that: - Similar inputs lead to
vastly different outputs - Make there no guarantee that an adversarial
NN's guess of \(x\) is closer to \(x+\epsilon_1\) than it is to
\(x+ \frac{1}{\epsilon_2}\) - As in, we can't train a NN to reverse it!
</p>

<p>
<b>Implementation --</b> One possible implementation of these permute layers
would be simply permuting the bits that make up our tensors.
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-proof-of-concept" class="outline-4">
<h4 id="proof-of-concept"><span class="section-number-4">1.1.3</span> Proof of concept</h4>
<div class="outline-text-4" id="text-proof-of-concept">
<p>
The code below is meant as proof of concept &#x2013; or rather, demonstration
of concept. It is messy, unoptimized, and surely error ridden, but it
seems to work.
</p>

<p>
<b>Outline:</b> - Create a deep neural network with randomly initialized
weights. - Ensure that it is deterministic with a set seed. This seed
could potentially be carried with the hash. - Add permute layers
throughout the model. - Store the first 8 bits to preserve the mantissa,
then concatenate and permute the rest. - Reform floats from the permuted
bits, and convert back into a tensor. - Do some post-processing to clean
the output - Convert each float in the output tensor to bits, then take
the last half of each as this is the part that is most shuffled -
Convert to base 10, then to 16 - Profit
</p>

<p>
The code can also be found
<a href="https://gist.github.com/TheEnquirer/1260b18f40cec198348a0a30d0a19e83">here</a>.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-comment-delimiter">#####################</span>
<span class="org-comment-delimiter">#       </span><span class="org-comment">SETUP       #</span>
<span class="org-comment-delimiter">#####################</span>

<span class="org-comment-delimiter"># </span><span class="org-comment">imports</span>
<span class="org-keyword">import</span> torch
<span class="org-keyword">import</span> torch.nn <span class="org-keyword">as</span> nn
<span class="org-keyword">import</span> torch.nn.functional <span class="org-keyword">as</span> F
<span class="org-keyword">import</span> math
<span class="org-keyword">import</span> struct
<span class="org-keyword">from</span> codecs <span class="org-keyword">import</span> decode
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> string

<span class="org-variable-name">INP</span> = [0.1, 0.0, 1.01] <span class="org-comment-delimiter"># </span><span class="org-comment">our input!</span>
<span class="org-variable-name">SAFE</span> = 8 <span class="org-comment-delimiter"># </span><span class="org-comment">don't permute the first 8 bits, so we don't get infs and 0</span>

<span class="org-comment-delimiter"># </span><span class="org-comment">makes things deterministic</span>
np.random.seed(0)
torch.manual_seed(0)

torch.set_default_dtype(torch.float64) <span class="org-comment-delimiter"># </span><span class="org-comment">make sure we use the right datatype!</span>


<span class="org-comment-delimiter">#######################</span>
<span class="org-comment-delimiter">#       </span><span class="org-comment">BASE NN       #</span>
<span class="org-comment-delimiter">#######################</span>

<span class="org-keyword">class</span> <span class="org-type">Net</span>(nn.Module): <span class="org-comment-delimiter"># </span><span class="org-comment">define the model</span>

    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>):
        <span class="org-builtin">super</span>(Net, <span class="org-keyword">self</span>).__init__()
        <span class="org-keyword">self</span>.<span class="org-variable-name">l1</span> = nn.Linear(3, 128) <span class="org-comment-delimiter"># </span><span class="org-comment">linear layer, with input size 3</span>
        <span class="org-keyword">self</span>.<span class="org-variable-name">pl1</span> = PermuteLayer(128,256) <span class="org-comment-delimiter"># </span><span class="org-comment">custom permute layer</span>

        <span class="org-keyword">self</span>.<span class="org-variable-name">l2</span> = nn.Linear(256, 512)
        <span class="org-keyword">self</span>.<span class="org-variable-name">pl2</span> = PermuteLayer(512, 512)

        <span class="org-keyword">self</span>.<span class="org-variable-name">l3</span> = nn.Linear(512, 256)
        <span class="org-keyword">self</span>.<span class="org-variable-name">pl3</span> = PermuteLayer(256, 256)

        <span class="org-keyword">self</span>.<span class="org-variable-name">l4</span> = nn.Linear(256, 128)
        <span class="org-keyword">self</span>.<span class="org-variable-name">pl4</span> = PermuteLayer(128, 8) <span class="org-comment-delimiter"># </span><span class="org-comment">output a tensor with 8 floats</span>


    <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x): <span class="org-comment-delimiter"># </span><span class="org-comment">run it through!</span>
        <span class="org-variable-name">x</span> = [100*(y+1) <span class="org-keyword">for</span> y <span class="org-keyword">in</span> x] <span class="org-comment-delimiter"># </span><span class="org-comment">add 1 and multiply by 100 for each input element</span>
        <span class="org-variable-name">x</span> = torch.tensor(x) <span class="org-comment-delimiter"># </span><span class="org-comment">then convert it to a tensor</span>

        <span class="org-variable-name">x</span> = <span class="org-keyword">self</span>.l1(x) <span class="org-comment-delimiter"># </span><span class="org-comment">run it through the layers</span>
        <span class="org-variable-name">x</span> = x.view(-1, 128)
        <span class="org-variable-name">x</span> = <span class="org-keyword">self</span>.pl1(x)
        <span class="org-variable-name">x</span> = <span class="org-keyword">self</span>.l2(x)
        <span class="org-variable-name">x</span> = <span class="org-keyword">self</span>.pl2(x)
        <span class="org-variable-name">x</span> = <span class="org-keyword">self</span>.l3(x)
        <span class="org-variable-name">x</span> = <span class="org-keyword">self</span>.pl3(x)
        <span class="org-variable-name">x</span> = <span class="org-keyword">self</span>.l4(x)
        <span class="org-variable-name">x</span> = <span class="org-keyword">self</span>.pl4(x)

        <span class="org-keyword">return</span> x


<span class="org-comment-delimiter">####################################</span>
<span class="org-comment-delimiter">#       </span><span class="org-comment">CUSTOM PERMUTE LAYER       #</span>
<span class="org-comment-delimiter">####################################</span>

<span class="org-keyword">class</span> <span class="org-type">PermuteLayer</span>(nn.Module): <span class="org-comment-delimiter"># </span><span class="org-comment">not my code! default linear code comes from https://auro-227.medium.com/writing-a-custom-layer-in-pytorch-14ab6ac94b77</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">after modification, acts as a normal linear layer except it permutes the bits.</span>
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, size_in, size_out):
        <span class="org-builtin">super</span>().__init__()
        <span class="org-keyword">self</span>.<span class="org-variable-name">size_in</span>, <span class="org-keyword">self</span>.<span class="org-variable-name">size_out</span> = size_in, size_out
        <span class="org-variable-name">weights</span> = torch.Tensor(size_out, size_in)
        <span class="org-keyword">self</span>.<span class="org-variable-name">weights</span> = nn.Parameter(weights)  <span class="org-comment-delimiter"># </span><span class="org-comment">nn.Parameter is a Tensor that's a module parameter.</span>
        <span class="org-variable-name">bias</span> = torch.Tensor(size_out)
        <span class="org-keyword">self</span>.<span class="org-variable-name">bias</span> = nn.Parameter(bias)

        <span class="org-comment-delimiter"># </span><span class="org-comment">initialize weights and biases</span>
        nn.init.kaiming_uniform_(<span class="org-keyword">self</span>.weights, a=math.sqrt(5)) <span class="org-comment-delimiter"># </span><span class="org-comment">weight init</span>
        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(<span class="org-keyword">self</span>.weights)
        bound = 1 / math.sqrt(fan_in)
        nn.init.uniform_(<span class="org-keyword">self</span>.bias, -bound, bound)  <span class="org-comment-delimiter"># </span><span class="org-comment">bias init</span>

    <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, x): <span class="org-comment-delimiter"># </span><span class="org-comment">where the permuting happens</span>
        <span class="org-comment-delimiter"># </span><span class="org-comment">this part isn't pretty..</span>
        <span class="org-comment-delimiter"># </span><span class="org-comment">but according to Dr. Brian Dean, we don't need to constant factor optimize!</span>

        bits = <span class="org-string">""</span> <span class="org-comment-delimiter"># </span><span class="org-comment">store bits in a char array</span>
        saved = [] <span class="org-comment-delimiter"># </span><span class="org-comment">save the bits we want to protect</span>

        <span class="org-keyword">for</span> i,v <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(x[0]): <span class="org-comment-delimiter"># </span><span class="org-comment">loop through the floats</span>
            tnsr = float_to_bin(v) <span class="org-comment-delimiter"># </span><span class="org-comment">convert them to binary</span>
            saved.append(tnsr[:SAFE]) <span class="org-comment-delimiter"># </span><span class="org-comment">save what we need to</span>
            bits += tnsr[SAFE:] <span class="org-comment-delimiter"># </span><span class="org-comment">and add to the char array</span>

        p = np.random.permutation([x <span class="org-keyword">for</span> x <span class="org-keyword">in</span> bits]) <span class="org-comment-delimiter"># </span><span class="org-comment">permute it!</span>
        p = <span class="org-string">''</span>.join(<span class="org-builtin">map</span>(<span class="org-builtin">str</span>, p)) <span class="org-comment-delimiter"># </span><span class="org-comment">and then.. join it back together</span>

        converted = []

        <span class="org-comment-delimiter"># </span><span class="org-comment">loop through p, chunk it into segments</span>
        <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(<span class="org-builtin">len</span>(p)//(64-SAFE)):
            <span class="org-comment-delimiter"># </span><span class="org-comment">convert segment to floats</span>
            item = bin_to_float(saved[i]+p[(64-SAFE)*i:((64-SAFE)*i)+(64-SAFE)])
            converted.append(item)

        converted = torch.tensor([converted]) <span class="org-comment-delimiter"># </span><span class="org-comment">change it back to a tensor</span>
        x = converted

        w_times_x= torch.mm(x, <span class="org-keyword">self</span>.weights.t()) <span class="org-comment-delimiter"># </span><span class="org-comment">matrix multiply them</span>
        <span class="org-keyword">return</span> torch.add(w_times_x, <span class="org-keyword">self</span>.bias)  <span class="org-comment-delimiter"># </span><span class="org-comment">w times x + b</span>


<span class="org-comment-delimiter">#########################</span>
<span class="org-comment-delimiter">#       </span><span class="org-comment">HELPERS         #</span>
<span class="org-comment-delimiter">#########################</span>

<span class="org-comment-delimiter"># </span><span class="org-comment">not my code! from https://stackoverflow.com/questions/16444726/binary-representation-of-float-in-python-bits-not-hex</span>
<span class="org-keyword">def</span> <span class="org-function-name">bin_to_float</span>(b):
    <span class="org-doc">""" Convert binary string to a float. """</span>
    bf = int_to_bytes(<span class="org-builtin">int</span>(b, 2), 8)  <span class="org-comment-delimiter"># </span><span class="org-comment">8 bytes needed for IEEE 754 binary64.</span>
    <span class="org-keyword">return</span> struct.unpack(<span class="org-string">'&gt;d'</span>, bf)[0]

<span class="org-keyword">def</span> <span class="org-function-name">int_to_bytes</span>(n, length):  <span class="org-comment-delimiter"># </span><span class="org-comment">Helper function</span>
    <span class="org-doc">""" Int/long to byte string.</span>
<span class="org-doc">        Python 3.2+ has a built-in int.to_bytes() method that could be used</span>
<span class="org-doc">        instead, but the following works in earlier versions including 2.x.</span>
<span class="org-doc">    """</span>
    <span class="org-keyword">return</span> decode(<span class="org-string">'%%0%dx'</span> % (length &lt;&lt; 1) % n, <span class="org-string">'hex'</span>)[-length:]

<span class="org-keyword">def</span> <span class="org-function-name">float_to_bin</span>(value):  <span class="org-comment-delimiter"># </span><span class="org-comment">For testing.</span>
    <span class="org-doc">""" Convert float to 64-bit binary string. """</span>
    [<span class="org-variable-name">d</span>] = struct.unpack(<span class="org-string">"&gt;Q"</span>, struct.pack(<span class="org-string">"&gt;d"</span>, value))
    <span class="org-keyword">return</span> <span class="org-string">'{:064b}'</span>.<span class="org-builtin">format</span>(d)


<span class="org-keyword">def</span> <span class="org-function-name">int2base</span>(x, base): <span class="org-comment-delimiter"># </span><span class="org-comment">not my code! modified from https://stackoverflow.com/questions/2267362/how-to-convert-an-integer-to-a-string-in-any-base</span>
    <span class="org-keyword">if</span> x &lt; 0:    sign = -1
    <span class="org-keyword">elif</span> x == 0: <span class="org-keyword">return</span> digs[0]
    <span class="org-keyword">else</span>:        sign = 1
    x *= sign
    digits = []
    <span class="org-keyword">while</span> x:
        digits.append(digs[x % base])
        x = x // base
    <span class="org-keyword">if</span> sign &lt; 0: digits.append(<span class="org-string">'-'</span>)
    digits.reverse()
    <span class="org-keyword">return</span> <span class="org-string">''</span>.join(digits)
digs = string.digits + string.ascii_letters

<span class="org-comment-delimiter">########################</span>
<span class="org-comment-delimiter">#       </span><span class="org-comment">OUTPUT         #</span>
<span class="org-comment-delimiter">########################</span>

model = Net()

result = <span class="org-builtin">list</span>(model(INP).detach().numpy()[0]) <span class="org-comment-delimiter"># </span><span class="org-comment">convert output to list</span>

output_bits = <span class="org-string">''</span>
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> result:
    <span class="org-comment-delimiter"># </span><span class="org-comment">convert to bits, then take the second half</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">because it's more shuffled</span>
    output_bits += float_to_bin(i)[32:]

<span class="org-keyword">print</span>(int2base(<span class="org-builtin">int</span>(output_bits, 2), 16)) <span class="org-comment-delimiter"># </span><span class="org-comment">clean the output up and print it out</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-results" class="outline-4">
<h4 id="results"><span class="section-number-4">1.1.4</span> Results</h4>
<div class="outline-text-4" id="text-results">
<p>
Running the hash function with a few example inputs gives us:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Input</th>
<th scope="col" class="org-left">Output</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">1.0, 0.0, 0.0</td>
<td class="org-left">7707b1d82074095cd7ec672e372dd54002594a60762ddab49954fd7536983af3</td>
</tr>

<tr>
<td class="org-left">1.0, 0.1, 0.0</td>
<td class="org-left">489014b1b6a164c4410abb09d38fa2c974dda663853a870d8da2e7bbe1276561</td>
</tr>

<tr>
<td class="org-left">1.0, 0.01, 0.0</td>
<td class="org-left">ba1c5e18233cfd68ef14fa7d77cf47fbaeca8182bcc6688fdfce32b0feab6e69</td>
</tr>

<tr>
<td class="org-left">1.0, 0.01, 1000.0</td>
<td class="org-left">593c5d758312de157e7bff4733227ddb95033a364724e8e7492a355ca32b56e0</td>
</tr>
</tbody>
</table>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Raw Output</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">6898.310410004538, -27752.31448079027, 5368.972044730368, 15157.626683930517, 2565.062517919854, 3530.8778547601196, -1500.8912099151323, 12012.064166096481</td>
</tr>

<tr>
<td class="org-left">10800.338151941398, 7383.0027867194185, 3447.3852618554415, 27197.54416266343, -16078.800441461795, 15308.535315814917, -10308.394947398328, 21108.498117302897</td>
</tr>

<tr>
<td class="org-left">6385.139558575604, -11019.165137885793, -2616.5486990505, -4771.384640650819, -27374.854418398354, -22029.83964691363, 32460.107410001277, 39128.374837604184</td>
</tr>

<tr>
<td class="org-left">28033.42732152106, 27429.429875103833, 19619.10146999292, -20498.237496016412, 28754.94659500775, 20997.47309229607, 11754.59598281484, 13.915845011749695</td>
</tr>
</tbody>
</table>

<p>
Cosine Similarity of these outputs were calculated with:
</p>

<pre class="example" id="org1e2c8ff">
similarity = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)
</pre>

<p>
The sorted Cosine Similarities of the outputs above are as follows:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">A</th>
<th scope="col" class="org-right">B</th>
<th scope="col" class="org-right">Similarity</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-right">4</td>
<td class="org-right">-0.29788</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-right">4</td>
<td class="org-right">-0.24353</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">4</td>
<td class="org-right">-0.09071</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">3</td>
<td class="org-right">0.157713</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">3</td>
<td class="org-right">0.240493</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">0.372453</td>
</tr>
</tbody>
</table>

<p>
This table demonstrates how different inputs lead to drastically
different outputs, regardless of how similar the inputs were.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">%%</th>
<th scope="col" class="org-right">A</th>
<th scope="col" class="org-right">B</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">2</td>
<td class="org-right">4</td>
<td class="org-right">0.000996</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">4</td>
<td class="org-right">0.000999</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-right">4</td>
<td class="org-right">0.001000</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">0.995037</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">3</td>
<td class="org-right">0.995982</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">3</td>
<td class="org-right">0.999950</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1.0</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">2</td>
<td class="org-right">1.000000</td>
</tr>
</tbody>
</table>

<p>
%% Further analysis is required, as collision rate and etc. have not yet
been determined because no large scale tests have been done. However,
this method of hash function seems potentially viable.
</p>
</div>
</div>
</div>
</div>
</div>
</body>
</html>
