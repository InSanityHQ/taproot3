<html><head><meta charset="utf-8" /><meta content="width=device-width, initial-scale=1.0" name="viewport" /><meta content="Taproot Authors" name="author" /><meta content="A collection of academic and project notes" name="description" /><title>Taproot</title><link href="/static/css/firn_base.css" rel="stylesheet" /><link href="/static/css/prism.css" rel="stylesheet" /><link href="/static/css/global.css" rel="stylesheet" /><script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js" type="text/javascript"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        'displayAlign': 'center',
        'displayIndent': '0em',
        'extensions': ['tex2jax.js'],
        'tex2jax': {
        'inlineMath': [ ['$','$'], ['\\(','\\)'] ],
        'processEscapes': true
        },
  loader: {load: ['[tex]/cases']},
  tex: {packages: {'[+]': ['cases']}},
        'HTML-CSS': { scale: 100,
                        linebreaks: { automatic: 'false' },
                        webFont: 'TeX'
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: 'false' },
              font: 'TeX'},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: 'AMS'},
               MultLineWidth: '85%',
               TagSide: 'right',
               TagIndent: '.8em'
             }})</script></head><body><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-core.min.js" type="text/javascript"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/autoloader/prism-autoloader.min.js" type="text/javascript"></script><div class="headerline"><a class="wordmark" href="https://taproot3.sanity.gq/" style="border:0">TR3</a></div><main><article class="content rss"><div class="preamble"><h1 class="title">Validation Part B</h1></div><div class="metamble"><span><span>Written by </span><span>Huxley</span></span></div><div class="notebody"><div><section><hr /></section><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="part-a"><span class="firn-headline-text"><span>Part A</span></span></h1><section><ul><li><p><span>What are the best and worst possible scores it could get?</span></p><ul><li><p><span>When using f1 score, the best possible score would be 1, and the
    worst would be 0.</span></p></li></ul></li><li><p><span>Why do you think the model is getting the score you're seeing?</span></p><ul><li><p><span>The score I'm seeing is 1. This is expected, as I am testing a
    decision tree on the iris data it was trained on.</span></p></li></ul></li><li><p><span>How do the scores between the training and testing sets compare?</span></p><ul><li><p><span>I would assume that the testing set score would have a lower f1
    score than the training set, as the model is specifically fitted for
    the training data.</span></p></li></ul></li></ul></section></div><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="part-b"><span class="firn-headline-text"><span>Part B</span></span></h1><section><p><span>Please answer the following questions and briefly explain your answer:</span></p><ul><li><p><span>Thinking about the R^2 metric used for evaluating regression, answer
  the following questions:</span></p><ul><li><p><span>What is the best highest possible score you could get?</span></p><ul><li><p><span>The highest possible score is 1, representing a perfect fit. In
      order to achieve an R^2 of 1, the error must be 0.</span></p></li></ul></li><li><p><span>If your model simply predicted the average value of the training set
    no matter what the input was, what score would you get on a test set
    whose average matched that of the training set?</span></p><ul><li><p><span>The score would be 0, as the denominator and numerator would be
      equal. This would result in the equation 1-1, which equals 0.</span></p></li></ul></li><li><p><span>What is the lowest score that you can get?</span></p><ul><li><p><span>Negative infinity.</span></p></li></ul></li></ul></li><li><p><span>When using accuracy to measure your model's performance on a
  classification problem:</span></p><ul><li><p><span>What is the best possible score you could get?</span></p><ul><li><ol><li><p><span>In a scenario with x samples, an entirely correct
         classification would lead to x/x, which is 1.</span></p></li></ol></li></ul></li><li><p><span>If your model always predicted the same class no matter what the
    input, what score would you get on a test set where 85% of the items
    were in that class?</span></p><ul><li><p><span>85%</span></p></li></ul></li></ul></li><li><p><span>What is the worst possible score you can get on a dataset that only
  has two classes?</span></p><ul><li><ol><li><p><span>Every sample could be sorted incorrectly as long as there is more
       than one class.</span></p></li></ol></li></ul></li><li><p><span>A model gets a recall score of 0 for class A on a test set with
  classes A, B, and C. If you take one of the test items that is in
  class A and have this model predict what class it is, what will it
  predict?</span></p><ul><li><p><span>A recall score of 0 for class A means that every item in class A was
    labeled incorrectly. When given another sample in class A, the model
    will predict either class B or class C.</span></p></li></ul></li><li><p><span>A model gets a precision score of 1 for class A on a test set with
  classes A, B, and C. If you take one of the test items that is in
  class A and have your model predict what class it is, what will it
  predict?</span></p><ul><li><p><span>The model could predict any of the three classes. A precision value
    is only effected by true positive and false positive rate, meaning
    that when given a sample in class A, predicting C or B would not
    lower class A's precision.</span></p></li></ul></li><li><p><span>If a model with classes A and B has an AUC score of 1 and you give it
  an item from the test set that is in class A, what class will it
  predict and what probability will it give for that class?</span></p><ul><li><p><span>It will predict class A, and give a probability of 100%. This is
    because an AUC score of 1 represents a perfect model.</span></p></li></ul></li><li><p><span>If a model with classes A and B has an AUC score of 0 and you give it
  an item from the test set that is in class A, what class will it
  predict and what probability will it give for that class?</span></p><ul><li><p><span>This means that the model is always perfectly incorrect. If you were
    to give it a sample from class B, it will always pick class A, and
    vise versa. The model would still give a probability of 100%.</span></p></li></ul></li></ul></section></div></div></div></article><div class="metapanel"><div class="metalabel">Contents</div><ol><li><a href="#part-a">Part A</a></li><li><a href="#part-b">Part B</a></li></ol><div class="metalabel">Backlinks</div><ul class="firn-backlinks"><li class="firn-backlink"><a href="/cs/intro_to_ml/KBMLCorrectionsBin">Ml Corrections Bin</a></li></ul></div></main></body></html>