<html><head><meta charset="utf-8" /><meta content="width=device-width, initial-scale=1.0" name="viewport" /><meta content="Huxley" name="author" /><meta content="A note on Taproot, a connected notes system." name="description" /><title>Neural Nets Assignment</title><link href="/static/css/firn_base.css" rel="stylesheet" /><link href="/static/css/prism.css" rel="stylesheet" /><link href="/static/css/global.css" rel="stylesheet" /><script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js" type="text/javascript"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        'displayAlign': 'center',
        'displayIndent': '0em',
        'extensions': ['tex2jax.js'],
        'tex2jax': {
        'inlineMath': [ ['$','$'], ['\\(','\\)'] ],
        'processEscapes': true
        },
        'HTML-CSS': { scale: 100,
                        linebreaks: { automatic: 'false' },
                        webFont: 'TeX'
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: 'false' },
              font: 'TeX'},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: 'AMS'},
               MultLineWidth: '85%',
               TagSide: 'right',
               TagIndent: '.8em'
             }})</script></head><body><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-core.min.js" type="text/javascript"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/autoloader/prism-autoloader.min.js" type="text/javascript"></script><div class="headerline"><a class="wordmark" href="https://taproot3.sanity.gq/" style="border:0">TR3</a></div><main><article class="content rss"><div class="preamble"><h1 class="title">Neural Nets Assignment</h1></div><div class="metamble"><span><span>Written by </span><span>Huxley</span></span></div><div class="notebody"><div><section><p><span>#ref #ret</span></p><hr /></section><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="opt-3-+"><span class="firn-headline-text"><span>Opt 3 +</span></span></h1><section><p><span>Looked at the</span><a class="firn-external" href="https://github.com/ml4a/ml4a-guides/blob/master/notebooks/convolutional_neural_networks.ipynb" target="_blank">Convolutional
Neural Network Notebook</a></p><ul><li><p><span>What type of data are they using?</span></p><ul><li><p><span>They are using images as their input data.</span></p></li></ul></li><li><p><span>What conversions (if any) had to be done to the data before it could
  be put into the neural network?</span></p><ul><li><p><span>For the basic neural network, they reshape the data to be individual
    vectors, make them float32, normalize the data, then convert the
    vectors to binary class matrices via one hot encoding. For the CNN,
    they repeat the previous steps except without reshaping the data
    into unrolled input vectors.</span></p></li></ul></li><li><p><span>What is the output of the neural network, both in terms of what it
  looks like to the computer (e.g. integers in the range [0-2]) and how
  humans should interpret it (e.g. the type of iris)?</span></p><ul><li><p><span>The output should be an array of probabilities for each category,
    which can be interpreted as, at a given index in the array, the
    item's probabilities for belonging in each category.</span></p></li></ul></li><li><p><span>How many hidden layers does the network have, and what type are they
  (e.g. fully connected, convolutional, recurrent, LSTM, sparse, etc.)?</span></p><ul><li><p><span>For the final iteration of the CNN, the model has four hidden layers
    --- two convolutional, and two dense.</span></p></li></ul></li><li><p><span>What activation function(s) does it use?</span></p><ul><li><p><span>The CNN used ReLU and softmax.</span></p></li></ul></li><li><p><span>What loss or cost function is it using?</span></p><ul><li><p><span>The model's loss function is categorical crossentropy</span></p></li></ul></li><li><p><span>What kind of validation (if any) are they using?</span></p><ul><li><p><span>The model uses accuracy as it's validation metric.</span></p></li></ul></li><li><p><span>What other validation methods might work for this type of problem?</span></p><ul><li><p><span>Any validation that works for classification problems, such as
    recall, f-measure, or precision.</span></p></li></ul></li><li><p><span>Why do you think the authors may have chosen this architecture for
  their network?</span></p><ul><li><p><span>They started with a small network to help illustrate the concepts
    more clearly, then gradually added more layers. They used a CNN so
    the model would be able to perform feature based recognition.</span></p></li></ul></li><li><p><span>Are there any changes you might try, if you were going to improve on
  their model?</span></p><ul><li><p><span>I will try to add some more layers, as well as some more dropout.</span></p></li></ul></li></ul></section></div><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="opt-3-pt 2!"><span class="firn-headline-text"><span>Opt 3 pt. 2!</span></span></h1><div class="firn-headline-section firn-headline-section-2"><h2 class="firn-headline firn-headline-2" id="iteration-one:-more-regularization"><span class="firn-headline-text"><span>Iteration One: More Regularization</span></span></h2><section><p><span>Tried to add more regularization with this model setup:</span></p><p><span>After </span><em><span>hours</span></em><span> of training, the model produced these results:</span></p><p><span>Which is roughly </span><em><span>one percent worse.</span></em></p><p><span>great.</span></p><hr /></section></div><div class="firn-headline-section firn-headline-section-2"><h2 class="firn-headline firn-headline-2" id="i2:-better-placed-regularization"><span class="firn-headline-text"><span>I2: Better Placed Regularization</span></span></h2><section><p><span>Once realizing I wasn't on a GPU, and trying out this new model setup:</span></p><p><span>It produced these results:</span></p><p><span>Which, when compared with the original results:</span></p><p><span>Is better!</span></p><hr /></section></div><div class="firn-headline-section firn-headline-section-2"><h2 class="firn-headline firn-headline-2" id="i3:-more-of-that"><span class="firn-headline-text"><span>I3: More of That</span></span></h2><section><p><span>Dialed up dropout again...</span></p><p><span>Which gave an even better score!</span></p><hr /></section></div><div class="firn-headline-section firn-headline-section-2"><h2 class="firn-headline firn-headline-2" id="i4:-more-layers"><span class="firn-headline-text"><span>I4: More Layers</span></span></h2><section><p><span>Our test accuracy and training accuracy are approaching each other.</span></p><hr /></section></div><div class="firn-headline-section firn-headline-section-2"><h2 class="firn-headline firn-headline-2" id="i(9?):-more-more-layers"><span class="firn-headline-text"><span>I(9?): More More Layers</span></span></h2><section><p><span>After quite some more experimenting, these were around the best results
I could get. I achieved this by adding two new layers, and a significant
amount more dropout from the original model. However, this process was
mostly guess and check. I assume that as I get more practice with this
kind of work and gain a deeper understanding, I will get better at
iterating more quickly and more effectively. Right now, however, I don't
know how those skills would manifest.</span></p></section></div></div></div></div></article><div class="metapanel"><div class="metalabel">Contents</div><ol><li><a href="#opt-3-+">Opt 3 +</a></li><li><a href="#opt-3-pt 2!">Opt 3 pt. 2!</a><ol><li><a href="#iteration-one:-more-regularization">Iteration One: More Regularization</a></li><li><a href="#i2:-better-placed-regularization">I2: Better Placed Regularization</a></li><li><a href="#i3:-more-of-that">I3: More of That</a></li><li><a href="#i4:-more-layers">I4: More Layers</a></li><li><a href="#i(9?):-more-more-layers">I(9?): More More Layers</a></li></ol></li></ol><div class="metalabel">Backlinks</div></div></main></body></html>