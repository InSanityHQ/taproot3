<html><head><meta charset="utf-8" /><meta content="width=device-width, initial-scale=1.0" name="viewport" /><meta content="Taproot Authors" name="author" /><meta content="A collection of academic and project notes" name="description" /><title>Taproot</title><link href="/static/css/firn_base.css" rel="stylesheet" /><link href="/static/css/prism.css" rel="stylesheet" /><link href="/static/css/global.css" rel="stylesheet" /><script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js" type="text/javascript"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        'displayAlign': 'center',
        'displayIndent': '0em',
        'extensions': ['tex2jax.js'],
        'tex2jax': {
        'inlineMath': [ ['$','$'], ['\\(','\\)'] ],
        'processEscapes': true
        },
        'HTML-CSS': { scale: 100,
                        linebreaks: { automatic: 'false' },
                        webFont: 'TeX'
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: 'false' },
              font: 'TeX'},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: 'AMS'},
               MultLineWidth: '85%',
               TagSide: 'right',
               TagIndent: '.8em'
             }})</script></head><body><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-core.min.js" type="text/javascript"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/autoloader/prism-autoloader.min.js" type="text/javascript"></script><div class="headerline"><a class="wordmark" href="https://taproot3.sanity.gq/" style="border:0">TR3</a></div><main><article class="content rss"><div class="preamble"><h1 class="title">Weight Agnostic Neural Networks</h1></div><div class="metamble"><span><span>Written by </span><span>Huxley</span></span></div><div class="notebody"><div><section><p><span>#ref #ret</span></p><hr /></section><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="weight-agnostic-neural-networks"><span class="firn-headline-text"><span>Weight Agnostic Neural Networks</span></span></h1><section><p><em><span>Or WANNN, for short.</span></em></p><p><span>My personal note's on</span><a class="firn-external" href="https://towardsdatascience.com/weight-agnostic-neural-networks-fce8120ee829" target="_blank">this
article</a><span>, and also</span><a class="firn-external" href="https://ai.googleblog.com/2019/08/exploring-weight-agnostic-neural.html" target="_blank">this
article</a><span>.</span></p><hr /><p><span>Animals can perform tasks when they are born without prior experience to
the world. If the brain is pre-wired, then learning new from experience
would cause a loss of the old skill. What gives?</span></p><p><span>WANNs can perform tasks regardless of the weights in its connections by
operating off of a pre-made structure.</span></p><p><span>Also, finding structures with inductive bias is hard and slow!</span></p></section><div class="firn-headline-section firn-headline-section-2"><h2 class="firn-headline firn-headline-2" id="neat"><span class="firn-headline-text"><span>NEAT</span></span></h2><section><p><em><span>NeuroEvolution of Augmented Topologies</span></em></p><p><span>Genetic algorithm in which mutations are done by changing the</span><strong><span>structure</span></strong><span> of the network.</span></p></section></div><div class="firn-headline-section firn-headline-section-2"><h2 class="firn-headline firn-headline-2" id="back-to-wann"><span class="firn-headline-text"><span>Back to WANN</span></span></h2><section><p><span>Can generalize the network to work with a range of weight values?</span></p><p><span>Instead of changing connection weights, they</span></p><ul><li><p><span>add connections,</span></p></li><li><p><span>add weight,</span></p></li><li><p><span>change activation functions.</span></p></li></ul><blockquote><p><span>  Networks in which the structure enables the task to be completed, not
  the weights, can be developed.</span></p></blockquote></section><div class="firn-headline-section firn-headline-section-3"><h3 class="firn-headline firn-headline-3" id="finding-wanns"><span class="firn-headline-text"><span>Finding WANNs</span></span></h3><section><p><span>Start with a small amount of network architectures then use NEAT on
them. With this system of growth and training, easier and more efficient
to optimize models across a wide range of input values.</span></p><p><span>Also optimizes for less complexity in the network.</span></p><p><span>Is general, but not as good at the individual scenarios as normal
networks.</span></p><p><strong><span>Proposed: WANN as starting point, then run normal training on the
network</span></strong></p><p><span>Also allows for much easier training as the structure generally has a
lot less nodes as it is specialized for a certain task.</span></p><p><span>Analogous to how animals learn.</span></p><p><span>Can also copy WANN network, and individually train, then use them in
collections for different input values?</span></p></section></div></div><div class="firn-headline-section firn-headline-section-2"><h2 class="firn-headline firn-headline-2" id="so?"><span class="firn-headline-text"><span>So?</span></span></h2><section><p><span>WANNs make models more interpretable, as their solutions or logic is
encoded directly into their structure.</span></p><p><span>More general, and deals better with varying inputs.</span></p><p><span>Also allows us to encode 'intellegence' from the creation of of the
network</span></p><p><span>Can be used to find 'building blocks,' sort of like automating the
finding of revolutionary structures like CNNs.</span></p></section></div></div></div></div></article><div class="metapanel"><div class="metalabel">Contents</div><ol><li><a href="#weight-agnostic-neural-networks">Weight Agnostic Neural Networks</a><ol><li><a href="#neat">NEAT</a></li><li><a href="#back-to-wann">Back to WANN</a><ol><li><a href="#finding-wanns">Finding WANNs</a></li></ol></li><li><a href="#so?">So?</a></li></ol></li></ol><div class="metalabel">Backlinks</div><ul class="firn-backlinks"><li class="firn-backlink"><a href="/history/history10/KBMlMasterIndex">Ml Master Index</a></li></ul></div></main></body></html>