<html><head><meta charset="utf-8" /><meta content="width=device-width, initial-scale=1.0" name="viewport" /><meta name="author" /><meta content="A note on Taproot, a connected notes system." name="description" /><title>Distributed Algorithms</title><link href="/static/css/firn_base.css" rel="stylesheet" /><link href="/static/css/prism.css" rel="stylesheet" /><link href="/static/css/global.css" rel="stylesheet" /><link href="/static/css/admonition.css" rel="stylesheet" /><script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js" type="text/javascript"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        'displayAlign': 'center',
        'displayIndent': '0em',
        'extensions': ['tex2jax.js'],
        'tex2jax': {
        'inlineMath': [ ['$','$'], ['\\(','\\)'] ],
        'processEscapes': true
        },
        'HTML-CSS': { scale: 100,
                        linebreaks: { automatic: 'false' },
                        webFont: 'TeX'
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: 'false' },
              font: 'TeX'},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: 'AMS'},
               MultLineWidth: '85%',
               TagSide: 'right',
               TagIndent: '.8em'
             }})</script></head><body><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-core.min.js" type="text/javascript"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/autoloader/prism-autoloader.min.js" type="text/javascript"></script><script crossorigin=="anonymous" src="https://kit.fontawesome.com/76c5ce8bda.js"></script><div class="headerline"><a class="wordmark" href="https://taproot3.sanity.gq/" style="border:0">TR3</a></div><main><article class="content rss"><div class="preamble"><h1 class="title">Distributed Algorithms</h1></div><div class="metamble"></div><div class="notebody"><div><section></section><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="what-are-distributed-algorithms?"><span class="firn-headline-text"><span>What are distributed algorithms?</span></span></h1><section><p><span>(https://stanford.edu/~rezab/dao/)
• Algorithms designed to run on multi-core processors by splitting up
into subroutines that simultaneously run on separate processors, saving
time. 
• The general field of algorithm hardware optimization includes
design algorithms to be run on GPUs and CPUs or even TPUs. -- TPU stands
for Tensor Processing Unit and is a chip developed by Google to aid
systems running TensorFlow</span></p></section></div><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="simple-example-of-distributed-algorithm-is-addition"><span class="firn-headline-text"><span>Simple example of distributed algorithm is addition</span></span></h1><section><p><span>• Normally one steps down the list and adds to running total
• Instead, with multiple cores, delegate pairs of numbers to each core and compute
the sum, reducing the list to n/2. Recursively repeat this until the
list is only one element to calculate the sum in O(log 2) time.</span></p></section></div><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="modelling"><span class="firn-headline-text"><span>Modelling</span></span></h1><section><p><span>• Normally the model is of the Random Access Machine, where a processor
is connected to memory and every operation takes a constant time step.
• Not as straightforward for distributed algorithms</span></p></section></div><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="multiprocessor-models"><span class="firn-headline-text"><span>Multiprocessor Models</span></span></h1><section><p><span>• Random Access Machine model but with additional processors. 
• Three seperate submodels with different methods of accessing memory. 
-- Local memory machine models each processor as having its own memory 
-- Modular memory machine models each processor as being routed to m processors 
-- PRAM models each processor as sharing memory with each other processor 
--- Benefits include accessing memory in single step 
--- Not widely believed that this model (in how it can access memory in single step) is
realistic</span></p></section></div><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="techniques"><span class="firn-headline-text"><span>Techniques</span></span></h1><div class="firn-headline-section firn-headline-section-2"><h2 class="firn-headline firn-headline-2" id="divide-and-conquer"><span class="firn-headline-text"><span>Divide and Conquer</span></span></h2><section><p><span>• Divide and Conquer (or splitting the problem into easier subproblems)
is useful in a sequential context, and extra useful in a parallel
context. 
-- Subproblems are usually independent of one another, and
therefore can benefit from being calculated in parallel. 
• Merge sort is one good example, as its usual time complexity is O(n log n)</span></p></section></div></div><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="in-class-lecture"><span class="firn-headline-text"><span>In-class Lecture</span></span></h1><section><p><span>• Around for ~25 years but more relevant in last 10
because many cores has become more of a reality. 
• Distribute tasks accross core</span></p></section><div class="firn-headline-section firn-headline-section-2"><h2 class="firn-headline firn-headline-2" id="area-1:-distributing-algorithms"><span class="firn-headline-text"><span>Area 1: Distributing Algorithms</span></span></h2><section><p><span> • Atomicity -- Either all operations are run or none of them so as to
 prevent error 
 • Leader election -- Sometimes a core needs to be a "leader" 
 • Consensus -- Processes must agreee</span></p></section></div><div class="firn-headline-section firn-headline-section-2"><h2 class="firn-headline firn-headline-2" id="area-2:-designing-algorithms-to-be-distributed"><span class="firn-headline-text"><span>Area 2: Designing Algorithms to be Distributed</span></span></h2><div class="firn-headline-section firn-headline-section-3"><h3 class="firn-headline firn-headline-3" id="map-reduce"><span class="firn-headline-text"><span>Map-Reduce</span></span></h3><section><p><span> • Map: (inkey , invalue ) -> list(outkey, intermediate value ) -- Write
 a function that gives each document to a different core, and count
 number of occurences in document. 
 • Reduce: Collect responses and combine 
 • Benefit is just structuring the problem in the framework</span></p></section></div></div></div></div></div></article><div class="metapanel"><div class="metalabel">Contents</div><ol><li><a href="#what-are-distributed-algorithms?">What are distributed algorithms?</a></li><li><a href="#simple-example-of-distributed-algorithm-is-addition">Simple example of distributed algorithm is addition</a></li><li><a href="#modelling">Modelling</a></li><li><a href="#multiprocessor-models">Multiprocessor Models</a></li><li><a href="#techniques">Techniques</a><ol><li><a href="#divide-and-conquer">Divide and Conquer</a></li></ol></li><li><a href="#in-class-lecture">In-class Lecture</a><ol><li><a href="#area-1:-distributing-algorithms">Area 1: Distributing Algorithms</a></li><li><a href="#area-2:-designing-algorithms-to-be-distributed">Area 2: Designing Algorithms to be Distributed</a><ol><li><a href="#map-reduce">Map-Reduce</a></li></ol></li></ol></li></ol><div class="metalabel">Backlinks</div></div></main></body></html>