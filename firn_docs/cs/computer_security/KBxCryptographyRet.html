<html><head><meta charset="utf-8" /><meta content="width=device-width, initial-scale=1.0" name="viewport" /><meta content="Huxley Marvit" name="author" /><meta content="A note on Taproot, a connected notes system." name="description" /><title>Cryptography</title><link href="/static/css/firn_base.css" rel="stylesheet" /><link href="/static/css/prism.css" rel="stylesheet" /><link href="/static/css/global.css" rel="stylesheet" /><link href="/static/css/admonition.css" rel="stylesheet" /><script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js" type="text/javascript"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        'displayAlign': 'center',
        'displayIndent': '0em',
        'extensions': ['tex2jax.js'],
        'tex2jax': {
        'inlineMath': [ ['$','$'], ['\\(','\\)'] ],
        'processEscapes': true
        },
        'HTML-CSS': { scale: 100,
                        linebreaks: { automatic: 'false' },
                        webFont: 'TeX'
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: 'false' },
              font: 'TeX'},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: 'AMS'},
               MultLineWidth: '85%',
               TagSide: 'right',
               TagIndent: '.8em'
             }})</script></head><body><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-core.min.js" type="text/javascript"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/autoloader/prism-autoloader.min.js" type="text/javascript"></script><script crossorigin=="anonymous" src="https://kit.fontawesome.com/76c5ce8bda.js"></script><div class="headerline"><a class="wordmark" href="https://taproot3.sanity.gq/" style="border:0">TR3</a></div><main><article class="content rss"><div class="preamble"><h1 class="title">Cryptography</h1></div><div class="metamble"><span><span>Written by </span><span>Huxley Marvit</span></span></div><div class="notebody"><div><section><p><span>#ret #hw</span></p><hr /></section><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="cryptography"><span class="firn-headline-text"><span>Cryptography</span></span></h1><section><p><span>Done prior:</span></p></section><div class="firn-headline-section firn-headline-section-2"><h2 class="firn-headline firn-headline-2" id="creating-a-custom-hash-function"><span class="firn-headline-text"><span>Creating a custom hash function</span></span></h2><div class="firn-headline-section firn-headline-section-3"><h3 class="firn-headline firn-headline-3" id="requirements"><span class="firn-headline-text"><span>Requirements</span></span></h3><section><p><span>What are the requirements for a hash function?</span></p><ul><li><p><a class="firn-external" href="https://stackoverflow.com/questions/2889473/when-is-it-safe-to-use-a-broken-hash-function" target="_blank">Source</a></p><ul><li><p><span>No preimage: given \(y\), it should not be feasible to find \(x\)
    such that \(h(x) = y\).</span></p></li><li><p><span>No second preimage: given \(x_1\), it should not be feasible to find
    \(x_2\) (distinct from \(x_1\)) such that \(h(x_1) = h(x_2)\).</span></p></li><li><p><span>No collision: it should not be feasible to find any \(x_1\) and
    \(x_2\) (distinct from each other) such that \(h(x_1) = h(x_2)\).</span></p></li></ul></li></ul></section></div><div class="firn-headline-section firn-headline-section-3"><h3 class="firn-headline firn-headline-3" id="nn-hash"><span class="firn-headline-text"><span>NN hash</span></span></h3><section><p><span>Intuitively, using a neural network as a hash function seems like it
won't work. In fact, I believe it doesn't work, </span><strong><span>I just don't know why
it doesn't work.</span></strong><span> </span><strong><span>*</span></strong><span> Of course, a vanilla neural network won't work
because we can just train a model to reverse its mapping. To solve this
problem, we can use something I call permute layers.</span></p></section><div class="firn-headline-section firn-headline-section-4"><h4 class="firn-headline firn-headline-4" id="permute-layers"><span class="firn-headline-text"><span>Permute Layers</span></span></h4><section><p><strong><span>Concept --</span></strong><span> Fundamentally, the concept of permute layers is to take an
input, and do some operation on it such that a non-continuous output
space is generated. The goal would be that: - Similar inputs lead to
vastly different outputs - Make there no guarantee that an adversarial
NN's guess of \(x\) is closer to \(x+\epsilon_1\) than it is to
\(x+ \frac{1}{\epsilon_2}\) - As in, we can't train a NN to reverse it!</span></p><p><strong><span>Implementation --</span></strong><span> One possible implementation of these permute layers
would be simply permuting the bits that make up our tensors.</span></p></section></div></div><div class="firn-headline-section firn-headline-section-3"><h3 class="firn-headline firn-headline-3" id="proof-of-concept"><span class="firn-headline-text"><span>Proof of concept</span></span></h3><section><p><span>The code below is meant as proof of concept -- or rather, demonstration
of concept. It is messy, unoptimized, and surely error ridden, but it
seems to work.</span></p><p><strong><span>Outline:</span></strong><span> - Create a deep neural network with randomly initialized
weights. - Ensure that it is deterministic with a set seed. This seed
could potentially be carried with the hash. - Add permute layers
throughout the model. - Store the first 8 bits to preserve the mantissa,
then concatenate and permute the rest. - Reform floats from the permuted
bits, and convert back into a tensor. - Do some post-processing to clean
the output - Convert each float in the output tensor to bits, then take
the last half of each as this is the part that is most shuffled -
Convert to base 10, then to 16 - Profit</span></p><p><span>The code can also be found</span><a class="firn-external" href="https://gist.github.com/TheEnquirer/1260b18f40cec198348a0a30d0a19e83" target="_blank">here</a><span>.</span></p><pre class="language-python"><code class="language-python">  #####################
  #       SETUP       #
  #####################

  # imports
  import torch
  import torch.nn as nn
  import torch.nn.functional as F
  import math
  import struct
  from codecs import decode
  import numpy as np
  import string

  INP = [0.1, 0.0, 1.01] # our input!
  SAFE = 8 # don't permute the first 8 bits, so we don't get infs and 0

  # makes things deterministic
  np.random.seed(0)
  torch.manual_seed(0)

  torch.set_default_dtype(torch.float64) # make sure we use the right datatype!


  #######################
  #       BASE NN       #
  #######################

  class Net(nn.Module): # define the model

      def __init__(self):
          super(Net, self).__init__()
          self.l1 = nn.Linear(3, 128) # linear layer, with input size 3
          self.pl1 = PermuteLayer(128,256) # custom permute layer

          self.l2 = nn.Linear(256, 512)
          self.pl2 = PermuteLayer(512, 512)

          self.l3 = nn.Linear(512, 256)
          self.pl3 = PermuteLayer(256, 256)

          self.l4 = nn.Linear(256, 128)
          self.pl4 = PermuteLayer(128, 8) # output a tensor with 8 floats


      def forward(self, x): # run it through!
          x = [100*(y+1) for y in x] # add 1 and multiply by 100 for each input element
          x = torch.tensor(x) # then convert it to a tensor

          x = self.l1(x) # run it through the layers
          x = x.view(-1, 128)
          x = self.pl1(x)
          x = self.l2(x)
          x = self.pl2(x)
          x = self.l3(x)
          x = self.pl3(x)
          x = self.l4(x)
          x = self.pl4(x)

          return x


  ####################################
  #       CUSTOM PERMUTE LAYER       #
  ####################################

  class PermuteLayer(nn.Module): # not my code! default linear code comes from https://auro-227.medium.com/writing-a-custom-layer-in-pytorch-14ab6ac94b77
      # after modification, acts as a normal linear layer except it permutes the bits.
      def __init__(self, size_in, size_out):
          super().__init__()
          self.size_in, self.size_out = size_in, size_out
          weights = torch.Tensor(size_out, size_in)
          self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.
          bias = torch.Tensor(size_out)
          self.bias = nn.Parameter(bias)

          # initialize weights and biases
          nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init
          fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)
          bound = 1 / math.sqrt(fan_in)
          nn.init.uniform_(self.bias, -bound, bound)  # bias init

      def forward(self, x): # where the permuting happens
          # this part isn't pretty..
          # but according to Dr. Brian Dean, we don't need to constant factor optimize!

          bits = "" # store bits in a char array
          saved = [] # save the bits we want to protect

          for i,v in enumerate(x[0]): # loop through the floats
              tnsr = float_to_bin(v) # convert them to binary
              saved.append(tnsr[:SAFE]) # save what we need to
              bits += tnsr[SAFE:] # and add to the char array

          p = np.random.permutation([x for x in bits]) # permute it!
          p = ''.join(map(str, p)) # and then.. join it back together

          converted = []

          # loop through p, chunk it into segments
          for i in range(len(p)//(64-SAFE)):
              # convert segment to floats
              item = bin_to_float(saved[i]+p[(64-SAFE)*i:((64-SAFE)*i)+(64-SAFE)])
              converted.append(item)

          converted = torch.tensor([converted]) # change it back to a tensor
          x = converted

          w_times_x= torch.mm(x, self.weights.t()) # matrix multiply them
          return torch.add(w_times_x, self.bias)  # w times x + b


  #########################
  #       HELPERS         #
  #########################

  # not my code! from https://stackoverflow.com/questions/16444726/binary-representation-of-float-in-python-bits-not-hex
  def bin_to_float(b):
      """ Convert binary string to a float. """
      bf = int_to_bytes(int(b, 2), 8)  # 8 bytes needed for IEEE 754 binary64.
      return struct.unpack('>d', bf)[0]

  def int_to_bytes(n, length):  # Helper function
      """ Int/long to byte string.
          Python 3.2+ has a built-in int.to_bytes() method that could be used
          instead, but the following works in earlier versions including 2.x.
      """
      return decode('%%0%dx' % (length << 1) % n, 'hex')[-length:]

  def float_to_bin(value):  # For testing.
      """ Convert float to 64-bit binary string. """
      [d] = struct.unpack(">Q", struct.pack(">d", value))
      return '{:064b}'.format(d)


  def int2base(x, base): # not my code! modified from https://stackoverflow.com/questions/2267362/how-to-convert-an-integer-to-a-string-in-any-base
      if x < 0:    sign = -1
      elif x == 0: return digs[0]
      else:        sign = 1
      x *= sign
      digits = []
      while x:
          digits.append(digs[x % base])
          x = x // base
      if sign < 0: digits.append('-')
      digits.reverse()
      return ''.join(digits)
  digs = string.digits + string.ascii_letters

  ########################
  #       OUTPUT         #
  ########################

  model = Net()

  result = list(model(INP).detach().numpy()[0]) # convert output to list

  output_bits = ''
  for i in result:
      # convert to bits, then take the second half
      # because it's more shuffled
      output_bits += float_to_bin(i)[32:]

  print(int2base(int(output_bits, 2), 16)) # clean the output up and print it out
</code></pre></section></div><div class="firn-headline-section firn-headline-section-3"><h3 class="firn-headline firn-headline-3" id="results"><span class="firn-headline-text"><span>Results</span></span></h3><section><p><span>Running the hash function with a few example inputs gives us:</span></p><table><tr><td><span>Input</span></td><td><span>Output</span></td></tr><tr></tr><tr><td><span>1.0, 0.0, 0.0</span></td><td><span>7707b1d82074095cd7ec672e372dd54002594a60762ddab49954fd7536983af3</span></td></tr><tr><td><span>1.0, 0.1, 0.0</span></td><td><span>489014b1b6a164c4410abb09d38fa2c974dda663853a870d8da2e7bbe1276561</span></td></tr><tr><td><span>1.0, 0.01, 0.0</span></td><td><span>ba1c5e18233cfd68ef14fa7d77cf47fbaeca8182bcc6688fdfce32b0feab6e69</span></td></tr><tr><td><span>1.0, 0.01, 1000.0</span></td><td><span>593c5d758312de157e7bff4733227ddb95033a364724e8e7492a355ca32b56e0</span></td></tr></table><table><tr><td><span>Raw Output</span></td></tr><tr></tr><tr><td><span>6898.310410004538, -27752.31448079027, 5368.972044730368, 15157.626683930517, 2565.062517919854, 3530.8778547601196, -1500.8912099151323, 12012.064166096481</span></td></tr><tr><td><span>10800.338151941398, 7383.0027867194185, 3447.3852618554415, 27197.54416266343, -16078.800441461795, 15308.535315814917, -10308.394947398328, 21108.498117302897</span></td></tr><tr><td><span>6385.139558575604, -11019.165137885793, -2616.5486990505, -4771.384640650819, -27374.854418398354, -22029.83964691363, 32460.107410001277, 39128.374837604184</span></td></tr><tr><td><span>28033.42732152106, 27429.429875103833, 19619.10146999292, -20498.237496016412, 28754.94659500775, 20997.47309229607, 11754.59598281484, 13.915845011749695</span></td></tr></table><p><span>Cosine Similarity of these outputs were calculated with:</span></p><p><span>The sorted Cosine Similarities of the outputs above are as follows:</span></p><table><tr><td><span>A</span></td><td><span>B</span></td><td><span>Similarity</span></td></tr><tr></tr><tr><td><span>1</span></td><td><span>4</span></td><td><span>-0.29788</span></td></tr><tr><td><span>3</span></td><td><span>4</span></td><td><span>-0.24353</span></td></tr><tr><td><span>2</span></td><td><span>4</span></td><td><span>-0.09071</span></td></tr><tr><td><span>2</span></td><td><span>3</span></td><td><span>0.157713</span></td></tr><tr><td><span>1</span></td><td><span>3</span></td><td><span>0.240493</span></td></tr><tr><td><span>1</span></td><td><span>2</span></td><td><span>0.372453</span></td></tr></table><p><span>This table demonstrates how different inputs lead to drastically
different outputs, regardless of how similar the inputs were.</span></p><table><tr><td><span>%%</span></td><td><span>A</span></td><td><span>B</span></td></tr><tr></tr><tr><td><span>2</span></td><td><span>4</span></td><td><span>0.000996</span></td></tr><tr><td><span>1</span></td><td><span>4</span></td><td><span>0.000999</span></td></tr><tr><td><span>3</span></td><td><span>4</span></td><td><span>0.001000</span></td></tr><tr><td><span>1</span></td><td><span>2</span></td><td><span>0.995037</span></td></tr><tr><td><span>2</span></td><td><span>3</span></td><td><span>0.995982</span></td></tr><tr><td><span>1</span></td><td><span>3</span></td><td><span>0.999950</span></td></tr><tr><td><span>1</span></td><td><span>1</span></td><td><span>1.0</span></td></tr><tr><td><span>2</span></td><td><span>2</span></td><td><span>1.000000</span></td></tr></table><p><span>%% Further analysis is required, as collision rate and etc. have not yet
been determined because no large scale tests have been done. However,
this method of hash function seems potentially viable.</span></p></section></div></div></div></div></div></article><div class="metapanel"><div class="metalabel">Contents</div><ol><li><a href="#cryptography">Cryptography</a><ol><li><a href="#creating-a-custom-hash-function">Creating a custom hash function</a><ol><li><a href="#requirements">Requirements</a></li><li><a href="#nn-hash">NN hash</a><ol><li><a href="#permute-layers">Permute Layers</a></li></ol></li><li><a href="#proof-of-concept">Proof of concept</a></li><li><a href="#results">Results</a></li></ol></li></ol></li></ol><div class="metalabel">Backlinks</div><ul class="firn-backlinks"><li class="firn-backlink"><a href="/cs/computer_security/index">CS240 Master Index</a></li><li class="firn-backlink"><a href="/cs/computer_security/KBxCryptography">Cryptography</a></li></ul></div></main></body></html>