:PROPERTIES:
:ID:       B7C0625B-655D-4DD7-9000-46629FBFD695
:END:
#+TITLE: Thanksgiving Axler Bonanza
#+AUTHOR: Houjun Liu

Hello, fellow person that comes across this. I have had one brief exposure with Linear Algebra following MATH 21-1 at UCSC. However, Axler is just so cool, so I am trying to learn a bit of linalg on the side to supplement my much more traditional linalg experience at the UC.

A few things of note. This whole thing is very "partial": in the sense that its contents contain many a parts of things omitted which I feel like I have a good grasp on from 21-1 such that I don't need to be reminded again; I only include things that maybe useful to me later either b/c I don't know it or I want to be reminded of it. As such, I don't think this will be helpful for most people.

* 1.A

** Things of Note
- $\lambda \in \mathbb{F}$ is called a "scalar". I mean duh but still.

*** Defining a list
A list of length $n$ is a collection of $n$ elements (any mathematical object?) separated by commas.

"Identical" lists are established when lists have:

- the same length
- same elements
- in the same order.

Its also called a $n$-tuple.

$n$ must be a finite non-negative value. Therefore, an "infinitely long list" is not a list.

*** Sets vs Lists
Lists have order and repetition. In sets, order and repetitions don't matter.

*** $\mathbb{F}$ 
- A set
- Containing 2 elements $0$, $1$
- Operators of "addition" and "multiplication" that satisfy the following properties

**** Properties of $\mathbb{F}$
That, with $\alpha, \beta, \lambda \in \mathbb{F}$:

- **Commutativity** $\alpha + \beta = \beta + \alpha$ and $\alpha \beta = \beta \alpha$
- **Associativity** $(\alpha + \beta) + \lambda = \alpha + (\beta + \lambda)$ and $(\alpha\beta)\lambda = \alpha(\beta\lambda)$
- **Existence of Identities** $\lambda + 0 = \lambda$ and $\lambda 1 = \lambda$
- **Additive Inverse** for every $\alpha$, $\exists \beta$ s.t. $\alpha + \beta = 0$
- **Multiplicative Inverse** for every $\alpha \neq 0$, $\exists \beta$ s.t. $\alpha \beta = 1$
- **Distribution** $\lambda(\alpha + \beta) = \lambda \alpha + \lambda \beta$

*** $\mathbb{F}^n$
\begin{equation}
  \mathbb{F}^n = \{(x_1, \ldots, x_n) : x_j \in \mathbb{F}\ for\ j=1,\ldots,n\}  
\end{equation}

We say $x_j$ is the $j^{th}$ coordinate of $(x_1, \dots,x_n)$.

In $\mathbb{F}^n$...

**** Addition
\begin{equation}
   (x_1,\dots,x_n) + (y_1,\ldots,y_n) = (x_1+y_1,\ldots,x_n+y_n) 
\end{equation}

**** Scalar Multiplication
\begin{equation}
    \lambda(x_1,\ldots,x_n) = (\lambda x_1, \ldots, \lambda x_n)
\end{equation}

**** Zero
\begin{equation}
   0=(0,\ldots,0)
\end{equation}

**** Additive Inverse
...of $x \in \mathbb{F}^n$:

\begin{equation}
   x+(-x) = 0
\end{equation}

That:

\begin{equation}
   x=(x_1, \ldots,x_n), -x=(-x_1,\ldots,-x_n) 
\end{equation}

** In-Text Exercises

*** Verify that $i^2 = -1$
$(0+1i)(0+1i) = (0 + 0 + 0 + i i) = -1$

*** Defining subtraction and division
$\alpha, \beta \in \mathbb{C}$

Subtraction could be defined in that:

- Let $-\alpha$ be defined as the additive inverse of $\alpha$
- Subtraction, therefore, is defined $\beta - \alpha = \beta + (-\alpha)$

Division could be defined in that:

- Let $1/\alpha$ be defined as the multiplicative inverse of $\alpha$
- Subtraction, therefore, is defined $\beta/\alpha = \beta(1/\alpha)$

** Actual Exercises

#+begin_quote
1: Suppose $a, b \in \mathbb{R}$, $a,b \neq 0$, find $c,d \in \mathbb{R}$ s.t. $\frac{1}{(a+bi)} = c+di$
#+end_quote

\begin{align}
    \frac{1}{(a+bi)} &= \frac{(a-bi)}{(a+bi)(a-bi)} =  \\
\Rightarrow & \frac{a-bi}{a^2-(bi)^2} = c+di \\
\Rightarrow & \frac{a-bi}{a^2+b^2} = c+di \\
\Rightarrow & \frac{a}{a^2+b^2}-\frac{bi}{a^2+b^2} = c+di 
\end{align}


Therefore:

\begin{equation}
    c = \frac{a}{a^2+b^2}
\end{equation}

\begin{equation}
    d = \frac{-b}{a^2+b^2}
\end{equation}

#+begin_quote
2: Show that $\frac{-1+\sqrt{3}i}{2}$ is the cube root of 1.
#+end_quote

\begin{align}
    &(\frac{-1+\sqrt{3}i}{2})^3 \\
\Rightarrow &(\frac{-1+\sqrt{3}i}{2})(\frac{-1+\sqrt{3}i}{2})(\frac{-1+\sqrt{3}i}{2})  \\
\Rightarrow &\frac{(-1+\sqrt{3}i)(-1+\sqrt{3}i)(-1+\sqrt{3}i)}{8}  \\
\Rightarrow &\frac{(1-2\sqrt{3}i-3)(-1+\sqrt{3}i)}{8} \\
\Rightarrow &\frac{(1-2\sqrt{3}i-3)(-1+\sqrt{3}i)}{8} \\
\Rightarrow &\frac{8}{8} = 1
\end{align}

#+begin_quote
3: Find two distinct square roots of $i$
#+end_quote

?

#+begin_quote
4: Show that $\alpha + \beta = \beta + \alpha, \forall \alpha,\beta \in \mathBB{C}$  
#+end_quote

Let:

$\forall a,b,c,d \in \mathbb{R}$

- $\alpha = (a+bi)$
- $\beta = (c+di)$

\begin{align}
   \alpha + \beta &= (a+bi) + (c+di) \\
&=(a+c) + (b+d)i \\
&=(c+a) + (d+b)i \\
&=(c+di) + (a+bi) \\
&=\beta + \alpha\ \blacksquare 
\end{align}

#+begin_quote
5: Show that $(\alpha + \beta) + \lambda = \alpha + (\beta+\lambda), \forall \alpha,\beta,\lambda \in \mathbb{C}$
#+end_quote

Let:

$\forall a,b,c,d,e,f \in \mathbb{R}$

- $\alpha = (a+bi)$
- $\beta = (c+di)$
- $\lambda = (e+fi)$

\begin{align}
   (\alpha + \beta)+\lambda &= ((a+bi) + (c+di))+(e+fi) \\
&=((a+c)+(b+d)i)+(e+fi) \\
&=(a+c+e)+(b+d+f)i \\
&=(a+(c+e))+(b+(d+f))i \\
&=(a+bi)+(c+e)+(d+f)i \\
&=(a+bi)+((c+di)+(e+fi)) \\
&=\alpha+(\beta+\lambda)\ \blacksquare 
\end{align}

* 1.B

** Things of Note

*** Vector Spaces $V$

A "vector"/"point" is a member of a vector space. The exact nature of scalar multiplication depends on which $\mathbb{F}$ we are working it; hence, when being precise, we say that $V$ is a vector space "over $\mathbb{F}$".

**** Motivation
- Addition is commutative, associative, and has identity
- Every element has additive inverse
- Scalar multiplication is associative
- Addition and scalar multiplication is connected by distribution

**** Basic Operators
- **Addition** on set $V$ is a function that assigns $u+v \in V$ to each pair $u,v \in V$
- **Scalar Multiplication** on set V is a function that assigns $\lambda v \in V$ to each $\lambda \in \mathbb{F}$ and $v \in V$. Note that this is different than a field, because if you can't multiply two different things in and out of the field and expect it to remain. But you could multiply an element in field $\mathbb{F}$ and a vector in vector space $V$ and expect it to stay in $V$.

Note also "Multiplication" is not defined as 1) there are two and 2) they behave very differently.

**** Properties
For $u,v,w \in V$ and $a,b \in \mathbb{F}$.

- **Commutativity** $u+v = v+u$
- **Associativity** $(u+v)+w = u+(v+w)$ and $(ab)v=a(bv)$. 
- **Additive Identity** $\exists 0\in V\ s.t.\ v+0 = v, \forall v\in V$
- **Additive Inverse** $\forall v \in V, \exists w\in V\ s.t.\ v+w=0$
- **Multiplicative Identity** $1v=v$ 
- **Distribution** $a(u+v) = au+av$ and $(a+b)v = av+bv$

**** Unique Additive Identity
The additive identity ("zero") in a vector space must be unique. (i.e. there cannot be two distinct zeros $0$ and $0'$ which both are $\in V$). This is because:

\begin{equation}
    0 = 0 + 0' = 0'+0 = 0'
\end{equation}

That --- if both $0$ and $0'$ are additive identities, $0=0'$.

**** Unique Additive Inverse
Every element in a vector space has an unique additive inverse (i.e. there cannot be two distinct additive inverses of $v \in V$ $w$ and $w'$ which both are $\in V$). 

Suppose $w$ and $w'$ are both additive inverses of $v$, then it holds that:

\begin{equation}
w = w+0 = w+(v+w') = (w+v)+w' = 0+w' = w'
\end{equation}

That --- if both $w$ and $w'$ exists in $V$, $w = w'$.

**** Zero and Vectors
$0v = 0$ for $v \in V$. $a\vec{0}=\vec{0}$ for $a \in \mathbb{F}$.

*** $\mathbb{F}^{\infty}$
Wait but aren't $\mathbb{F}^n$ supposed to be made of lists, which has finite length?

I guess its just sequences of all of everything in $F$.

\begin{equation}
   \mathbb{F}^{\infty} = \{(x_1, x_2,\ldots):x_j\in \mathbb{F}\ for\ j=1,2,\ldots \} 
\end{equation}

*** $\mathbb{F}^S$
$\mathbb{F}^S$ is defined as the set of functions that maps elements in set $S$ to $\mathbb{F}$. It is a vector space.

**** Addition
Addition between $f,g \in \mathbb{F}^S$ is defined by:

\begin{equation}
   (f+g)(x) = f(x)+g(x),\ \forall x \in S
\end{equation}

**** Scalar Multiplication
Multiplication between $\lambda \in \mathbb{F}$ and $f \in \mathbb{F}^s$, $\lambda f \in \mathbb{F}^s$ is defined as:

\begin{equation}
    (\lambda f)(x) = \lambda f(x),\ \forall x \in S
\end{equation}

**** $\mathbb{F}^n$ and $\mathbb{F}^\infty$ are special cases of $\mathbb{F}^s$
...this is because a list $\{x_1, x_2, x_3, \ldots, x_n\}$ is actually a bijective mapping between $\{1,2,3,\ldots,n\}$ (the indexes) and the values of the list, which are all $\in \mathbb{F}$. so :tada:!

** In-Text Exercises

*** Verify that $\mathbb{F}^n$ is a vector space over $\mathbb{F}$
Not going to write this one out, but:

- Commutativity: via rules addition, commutation (in $\mathbb{F}$), then undoing addition
- Associativity: addition, communication, then undoing addition
- Additive Identity: addition + definition of "zero" in $\mathbb{F}^n$
- Additive Inverse: addition + additive inverse (in $\mathbb{F}$)
- Multiplicative Identity: scalar multiplication (by 1) and then identity (in $\mathbb{F}$)
- Distribution: definition of addition in $\mathbb{F}^n$, scalar multiplication, undoing definition of addition again

** Actual Exercises

#+begin_quote
1: Proof that $-(-v) = v,\ \forall v \in V$
#+end_quote

| Step                 | Explanation          |
|----------------------+----------------------|
| $v = v+0$            | Additive identity    |
| $v = v+(-v+-(-v))$   | Additive inverse     |
| $v = (v+-v) + -(-v)$ | Associative property |
| $v = 0 + -(-v)$      | Additive inverse     |
| $v = -(-v)\ \blacksquare$        | Additive Identity    |

#+begin_quote
2: Suppose $a \in \mathbb{F}, v\in V$, and $av=0$. Proof $a=0$ or $v=0$.
#+end_quote

Let $a \neq 0$. We define the multiplicative inverse of $a$ as $a^{-1}$.

| Step                 | Explanation             |
|----------------------+-------------------------|
| $v = 1v$             | Multiplicative identity |
| $v = a a^{-1} v$     | Multiplicative inverse  |
| $v = av a^{-1}$      | Commutativity           |
| $v = 0 a^{-1}$       | Given                   |
| $v = 0\ \blacksquare$ | Number times 0          |

If $a=0$, $\blacksquare$.

#+begin_quote
3: Suppose $v,w \in V$, explain why $\exists$ unique $x\in V$ s.t. $v+3x = w$ 
#+end_quote

Let $x = \frac{1}{3}(w-v)$; by addition and scalar multiplication, $\exists x \in V$.

| Step                          | Explanation                    |
| $v + 3x = w$                  | Given                          |
| $v + 3(\frac{1}{3}(w-v)) = w$ | Defined                        |
| $v + (w - v) = w$             | Multiplication in $\mathbb{F}$ |
| $v - v + w = w$               | Commutativity                  |
| $w = w\ \blacksquare$       | Additive Inverse               |

Therefore, $\exists x \in V$ that satisfies the needed property.

Suppose there exists more than 1 $x$ which satisfies this property. We call them $x$ and $x'$. This would tell us the following equalities:

$v + 3x = w$, $v+3x' = w$.

It follows from the equalities that:

$3x = w-v$, $3x' = w-v$

Then, it follows that

$3x = 3x'$

Therefore:

\begin{equation}
x = x'
\end{equation}

Hence, if given that there exists $v + 3x = w$, $v+3x' = w$, $x=x'$. Hence, there is only one unique $x$ such that $v+3x = w$.

* 1.C
Axler, in his infinite wisdom, has crammed everything that's interesting to note in Chapter 1.c.

** Things of Note

*** Subspaces
(Woo hoo!)

A subset $U \subset V$ is called a "subspace of $V$" if $U$ is also a vector space using the same addition and scalar multiplication operators.

**** Checking for Subspaces
Check for three conditions:

For $U \subset V$

- **Additive Identity** $0 \in U$. (also could be defined as "set is nonempty", b/c if nonempty, and its closed under scalar multiplication, multiplying any element by $0$ will do the trick. But often showing $0$ is in it is actually simpler.)
- **Closed Under Addition** $u,w \in U$ implies $u+w \in U$
- **Closed Under Scalar Multiplication** $a \in \mathbb{F}$ and $u \in U$ implies $au \in U$

*** Summing Subsets
Suppose $U_1,\ldots,U_m$ are subsets of $V$. The "sum" of the subsets ($U_1 + \cdots + U_m$) is the set of all possible sums of elements in $U_1, \lodts, U_m$. That is:

\begin{equation}
   U_1 + \cdots + U_m = \{u_1 + \cdots + u_m : u_1 \in U_1, \ldots, u_m \in U_m \} 
\end{equation}

**** Properties of the Sums of Subspaces
Suppose $U_1, \ldots, U_m$ are subspaces of $V$. $U_1 + \cdots + U_m$ is the smallest subspace of $V$ containing all of $U_1,\ldots,U_m$.

*** Direct Sum

Suppose $U_1, \ldots, U_m$ are subspaces of $V$

- Sum $U_1 + \cdots + U_m$ is a direct sum if every element $u \in U_1 + \cdots + U_m$ can be only written only one way as a sum $u_1 + \cdots + u_m$
- Direct sum is noted as $U_1 \oplus \cdots \oplus U_m$ 

"Sums is the union, direct sums is the disjoint union".

**** Checking for Direct Sums
Suppose $U_1, \ldots, U_m$ are subspaces of $V$. $U_1 + \cdots + U_m$ is a direct sum iff the only way to write $0$ as a sum $u_1 + \cdots + u_m$ is by taking each $u_j$ equaling to 0. This could be implied from the definition of a direct sum: that there is only one way to write a $0$ as $u_1 + \cdots + u_m$, and being closed scalar multiplication means that you could multiply $0$ to each subspace individually and they still have to add up to $0$. 

**** Direct Sum of Two Subspaces
Suppose $U,W$ are subspaces of $V$.

$U+W$ is a direct sum iff $U \cap W = \{0\}$. 

** In-Text Excercises

*** Summing Subspaces, an Example
Suppose $U = \{(x,x,y,y) \in \mathbb{F}^4: x,y \in \mathbb{F}\}$ and $W= \{(x,x,x,y) \in \mathbb{F}^4 : x,y \in \mathbb{F}\}$. Then:

\begin{equation}
    U+W = \{(x,x,y,z) \in \mathbb{F}^4 : x,y,z \in \mathbb{F}\}
\end{equation}

We verify this by writing out the sum.

\begin{align}
    U + W &= \{(x_u,x_u,y_u,y_u) + (x_w,x_w,x_w,y_w) : x_u, y_u \in U, x_w, y_w \in W\} \\
&= \{(x_u+x_w,x_u+x_w,y_u+x_w,y_u+y_w) : x_u, y_u \in U, x_w, y_w \in W\} \\
&= \{\left(\underbrace{x}_{x_u+x_w},x,\underbrace{y}_{y_u+x_w},\underbrace{z}_{y_u+y_w}\right) : x,y,z \in \mathbb{F}\}
\end{align}

*** Verify sums equal to spaces
Suppose $U,W$ are subspaces of $\mathbb{F}^3$.

\begin{align}
    U =&\,\{(x,y,0) \in \mathbb{F}^3:x,y \in \mathbb{F}\} \\
    W =&\,\{(0,0,z) \in \mathbb{F}^3: z \in \mathbb{F}\}
\end{align}

Verify $\mathbb{F}^3 = U \oplus W$

\begin{equation}
  U + W = \{(x,y,z) \in \mathb{F}^3 : x,y,z \in \mathhb{F}\} = \mathbb{F}^3
\end{equation}

:tada:?

** Actual Excercises

#+begin_quote
1: For each of the following subsets of $\mathbb{F}^3$, determine if it is a subspace of $\mathbb{F}^3$.
#+end_quote

\begin{equation}
  U = \{(x_1, x_2, x_3) \in \mathbb{F}^3:x_1+2x_2+3x_3 = 0\}  
\end{equation}

We could see that $(0,0,0) \in U$.

Let $u_1 = (x_1, x_2, \frac{-x_1 - 2x_2}{3}}), u_2 = (x_3, x_4, \frac{-x_3 - 2x_4}{3}}). u_1, u_2 \in U$.

$u_1 + u_2 = (x_1+x_3,x_2+x_4,\frac{-(x_1+x_3)-2(x_2+x_4)}{3})$. Define $x_1 + x_3 = x$, $x_2+x_4 = y$. Therefore: $u_1 + u_2 = (x,y,\frac{-x-2y}{3})$.

$x+2y-x-2y = 0$.

Therefore, $U$ is closed under addition.

Let $u_1 = (x_1, x_2, \frac{-x_1 - 2x_2}{3}})$. We scale each element by scalar factor $\lambda$.

$\lambda u_1 = (\lambda x_1, \lambda x_2, \lambda \frac{-x_1 - 2x_2}{3}})$.

$\lambda x_1 + 2\lambda x_2 - \lambda x_1 -2 \lambda x_2 = 0$. Therefore, $U$ is closed under scalar multiplication.

Therefore, it is a subspace of $\mathbb{F}^3$.

#+begin_quote
10: Suppose that $U_1$ and $U_2$ are subspaces of $V$. Prove that the intersection $U_1 \cap U_2$ is a subspace of $V$. 
#+end_quote

Given $U_1$ and $U_2$ are both subspaces, $0$ must be in both subsets hence it must be in their intersection.

Let $u_1 = (x_1,y_1,z_1)$. Let $u_2 = (x_2,y_2,z_2)$. Finally, let $u_1,u_2 \in U_1 \cap U_2$. By this last fact, we could derive the fact that $u1,u2 \in U_1, U_2$.

As $U_1$ is a subspace, $U_1$ closed under addition. Since $u_1,u_2 \in U_1$, $u_1 + u_2 \in U_1$. As $U_2$ is a subspace, $U_2$ closed under addition. Since $u_1,u_2 \in U_2$, $u_1 + u_2 \in U_2$.

As $u_1 + u_2 \in U_1,U_2$, it is additionally $u_1 + u_2 \in U_1 \cap U_2$. $U_1 \cap U_2$ is therefore closed under addition.

By the same token....

As $U_1$ is a subspace, $U_1$ is closed under scalar multiplication. Since $u_1 \in U_1$, $\lambda u_1 \in U_1$. As $U_2$ is a subspace, $U_2$ is closed under scalar multiplication. Since $u_1 \in U_2$, $\lambda u_1 \in U_2$.

Therefore, as $\lambda u_1 \in U_1, U_2$, it is additionally true that $\lamba u_1 \in U_1 \cap U_2$, it is therefore closed under multiplication.

* 2.A 
What's with the balancing of these chapters? Like Span and Linear Independence is squished in one, but then Bases gets a whole chapter and so does dimension.

** Things of Note
- Lists of vectors are usually denoted as a list without parentheses 

*** Linear Combination
A linear combination of a list is a sum of vectors in the form:

\begin{equation}
   a_1 v_1 + \cdots + a_m v_m
\end{equation}

where, $a_1, \ldots, a_m \in \mathbb{F}$.

**** Verifying Linear Combinations
You could note that a linear combination is actually just a linear system of equations. That:

To check $(x,y,z)$ is a linear combination of $(v_1, v_2, v_3)$, $(w_1, w_2, w_3)$, figure if there exists a pair $(a_1, a_2)$ such that...

\begin{equation}
    \begin{cases}
x = a_1v_1 + a_2w_1 \\
y = a_1v_2 + a_2w_2 \\
z = a_1v_3 + a_2w_3 \\
    \end{cases}
\end{equation}

If so, there exists the requisite scalars such that the linear combination is possible.

*** Linear Span
The span is the set of all linear combinations of a list of vector.  That:

\begin{equation}
   span(v_1, \ldots, v_m) = \{a_1v_1 + \cdots + a_mv_m : a_1, \ldots, a_m \in \mathbb{F}\} 
\end{equation}

The span of an empty list of vectors $()$ is defined to be $\{0\}$. Due to the fact that any subspace must be closed under scalar multiplication + addition, the span of a list of vectors in $V$ is the smallest subspace of $V$ containing all the vectors in that list.

**** Spanning List
If $span(v_1, \ldots, v_m) = V$, we say that $v_1, \ldots, v_m$ spans $V$.

**** Finite-Dimensional Vector Space
A vector space is "finite-dimensional" if some list of vectors in it could span the space. i.e. there exists a list of vectors that span the space. Otherwise, it is called "infinite dimensional".

Every subspace of a finite dimentional vector space is finite dimentional.

*** Polynomials
We quickly recap the definition of a polynomial. A function $p: \mathbb{F} \to \mathbb{F}$ is called a polynomial if $\exists a_0, \ldots, a_m \in \mathbb{F}$ s.t.

\begin{equation}
   p(z) = a_0 + a_1 z + a_2 z ^2 + \cdots + a_mz^m, \forall z \in \mathbb{F}
\end{equation}

By the same token, $\mathcal{P}(\mathbb{F)$ is the set of all polynomials whereby the coefficients are in $\mathbb{F}$. This is a subspace of $\mathbb{F}^{\mathbb{F}}$, as verified below.

**** Degree of a Polynomial
A polynomial $p \in \mathcal{P}(\mathbb{F})$ has degree m if $\exists a_0, a_1, \ldots, a_m \in \mathbb{F}$ with $a_m \neq 0$ such that...

\begin{equation}
    p(z) = a_0 + a_1z + \cdots + a_mz^m, \forall z \in \mathbb{F} 
\end{equation}

If $p$ has degree $m$, we write $deg\ p = m$. A zero-polynomial is said to have degree $-\infty$.

**** $\mathcal{P}_m(\mathbb{F})$
For a non-negative integer $m$, $\mathcal{P}_m(\mathbb{F})$ is defined as the set of all polynomials with coefficients in $\mathbb{F}$ and degree at most $m$.

You will therefore notice, then, that:

\begin{equation}
    \mathcal{P}_m(\mathbb{F}) = span(1, z, \ldots, z^m)
\end{equation}

*** Linear Independence
Linear independence exists by a only /unique/ choice of scalars $a_1, \ldots, a_m$ to form any given $v \in span(v_1, \ldots, v_m)$. This could also be stated as that:

that the only choice $a_1, \ldots, a_m \in \mathbb{F}$  that makes $a_1v_1+ \cdots + a_mv_m = 0$ is the "trivial" case whereby $a_1 = \cdots = a_m = 0$.

The empty set is also defined as linearly independent.

*** Linear Dependence
A list of vectors in $V$ is linearly dependent if its not linearly independent.

A list $v_1, \ldots, v_m$ of vectors is linearly dependent if there exist $a_1, \ldots, a_m \in \mathbb{F}$ that's not all $0$, such that $a_1v_1 + \cdots + a_m v_m = 0$.

**** Linear Dependence Lemma
Suppose $v_1, \ldots, v_m$ is an linearly dependent list in $V$. Then, exists a $j \in \{1,2,\ldots, m\}$ such that:

1. $v_j \in span(v_1, \ldots, v_{j-1})$
2. If the $j^{th}$ term is removed, the span remains the same

**** Lengths of Lin. Indp List
In a finite-dimentional vector space, the length of every linearly independent list is less than or equal to the length of every spanning list.

** In-Text Excercises

*** $\mathcal{P}(\mathbb{F})$ is a subspace of $\mathbb{F}^{\mathbb{F}}$
Zero exists in the set as, for a polynomial, $(a_0, \ldots, a_m) = (0, \ldots, 0)$ would create a function $f: \mathbb{F} \to 0$.

Due to commutativity, we could group and factor-out input-variable $z$ such that the sum of two polynomials become $(a_0_a + a_0_b) + (a_1_a + a_1_b)z + \cdots + (a_m_a + a_m_b)z^m$, which would be another polynomial. This would be closed under addition.

Due to distribution, a scalar $\lambda$ multiplied to a polynomial would just scale every value by $\lambda$ resulting in $\lambda a_0 + \lambda a_1 z + \cdots + \lambda a_m z^m$, which would be another polynomial. This would be closed under scalar multiplication. 

Therefore, the set of polynomials in $\mathbb{F}$ is as subspace.

*** Show that $\mathcal{P}(\mathbb{F})$ in infinite-dimensional
Any list $U \subset \mathcal{P}(\mathbb{F})$ would contain a highest-degree polynomial with degree $m$. Therefore, the element $z^{m+1} \in \mathcal{P}(\mathbb{F})$ would not be in the span of the list: making the list not span the entire space. Therefore, there could not be a list that spans $\mathcal{P}(\mathbb{F})$, making it infinite-dimentional.

* 2.B

** Things of Note

(wow that was short.)

*** Bases
A basis of $V$ is a list of vectors in $V$ that's linearly independent *and* spans $V$. A list $v_1, \ldots, v_n$ could only be a basis if and only if every vector $v \in V$ could be written as a linear combination of $v_1, \ldots, v_n$ uniquely. The "uniquely" part makes linear independence, the "could be written" part makes span.

**** Reducing a Basis
Every spanning list in a vector space could be reduced to a basis of the vector space. Basically by removing everything that makes the span linearly dependent.

And therefore, every finite-dimentional vector space has a basis. Because you could just get a span of the space and keep reducing.

**** Building a Basis
The dual of the previous condition is also true. Take a linearly independent list, keep adding vectors that still maintains the linearly independent-ness of the list, and at some point you will get a basis.

As an correlary to this, we could claim that any subspace $U$ of $V$ is part of the a direct sum that adds up to $V$. This is because we could expand ("pad") $u_1, \ldots, u_m \in U$ with vectors $w_1,\ldots, w_n$ until we form a basis of $V$. We could see that $V = U+W$ because the collection of the vectors that make them up is the basis of $V$. And also because of that they are linearly independent.

because they are linearly independent, there is only one way to write $u_1, \ldots, u_m$ and $w_1, \ldots, w_n$ as a linear combination, making it a direct sum.

** In-Text Exercises

*** The list $1,z,\ldots,z^m$ is a basis for $\mathcal{P}_m(\mathbb{F})$
We proof first that the sequence spans $\mathcal{P}_m(\mathbb{F})$. This is easy to see, because, definitionally, a polynomial in $\mathcal{P}_m(\mathbb{F})$ a made of a linear combination of $1,z,\ldots,z^m$.

This list is furthermore linearly independent as, the only set of scalars $a_1, \ldots, a_{m+1}$ by which $1,z,\ldots,z^m$ could be scaled to be $0$ is in the case by which all scalars are $0$, making this list linearly independent.

* 2.C

** Things of Note

*** Dimension!
$dim\ V$ of a finite-dimentional vector space is the length of any basis of the space. Because the length of linear independent list must be $\leq$ spanning list, there is only one length that's possible in a space such that a list is a basis (linear independent AND spanning.)

- $dim\ \mathbb{F}^n = n$ because the standard basis $\mathbb{F}^n$ as length $n$
- $dim\ \mathcal{P}_m(\mathbb{F}) = m+1$ because the basis of the space $1,z,\ldots,z^m$ has $m+1$ elements

**** Subspace Dimension
If $V$ has a finite dimension + if $U$ is a subspace of $V$, $dim\ U \leq dim \ V$.

**** A John McHugh Special: "Half is good enough"
Linearly independent and/or spanning lists of vectors in $V$ with length $dim\ V$ is a basis of $V$. So proving two (length, basis, linearly indep., spanning) proves all four.

**** Dimension of a Sum
If $U_1$ and $U_2$ are finite-dimentional, then:

\begin{equation}
  dim(U_1 + U_2) = dim\ U_1 + dim\ U_2 - dim(U_1 \cap U_2) 
\end{equation}

** In-Text Excercises

*** Show that $1$, $(x-5)^2$, $(x-5)^3$ is a basis of the subspace $U$ of $\mathcal{P}_3 (\mathbb{R})$
Define $U$, as given by the question:

\begin{equation}
U = \{p \in \mathcal{P}_3(\mathbb{R}) : p'(5) = 0\}    
\end{equation}

By thinking a little hard, we could see that $1, (x-5)^2, (x-5)^3 \in U$. A linear combination of these elements are shown:

\begin{equation}
    a + b(x-5)^2 + c(x-5)^3 = 0
\end{equation}

We could see that the degree of $0$ is $0$. Therefore the coefficient for the $x^3$ term is $0$. As the 3rd-degree term in the left expression is $cx^3$, we therefore deduct that $c=0$. We repeat this to find $b=0, a=0$. Therefore, the list as prescribed is linearly independent. Furthermore, we could see that the dimension of $U$ would have to be $\geq 3$ as the length of this linearly independent list is $3$. As $U$ is a subspace of $\mathcal{P}_3$, $U$ has a dimension $\leq 4$ as the dimension of $\mathcal{P}_3$ is $4$ (bases $1, z, \ldots, z^3$).

We further show that, due to the fact that $U \neq \mathcal{P}_3(\mathbb{R})$ (i.e. for instance, $x^2 \in \mathcal{P}_3(\mathbb{R}), x^2 \not\in U$), and if $dim\ U = 4$, extending the bases of $U$ towards that for $\mathcal{P}_3(\mathbb{R})$ would exceed $dim\ \mathcal{P}_3(\mathbb{R})$. Therefore, $dim\ U = 3$.

Given the list as given is an linearly independent list with a length of $3$ in a subspace of dimension 3, the list is a basis of $U$.

* 3.A

** Things of Note

*** Linear Maps
A linear map form $V$ to $W$ is a function $T: V \to W$ which holds...

- **Commutativity**: $T(u+v) = Tu + Tv, \forall u,v \in V$
- **Homogeneity** $T(\lambda v) = \lambda(Tv), \forall \lambda \in \mathbb{F}, v \in V$

A set of all linear maps from $V \to W$ is denoted as $\mathcal{L}(V,W)$.

**** Zero
Let the symbol $0$, in the context of linear maps define a function that takes a element in $V$ and maps it to the additive identity in $W$. That is

\begin{equation}
    0v = 0
\end{equation}

**** Identity
The identity map, denoted in $I$, is the function that maps each element to itself.

\begin{equation}
   Iv = v, I \in \mathcal{L}(V,V)
\end{equation}

Note that this only makes sense if $I$ maps from one vector space back to the same space.

**** Differentiation
Define a $D \in \mathcal{L}(\mathcal{P}(\mathbb{R}), \mathcal{P}(\mathbb{R}))$ such that...

\begin{equation}
Dp = p'    
\end{equation}

The map $D$ is therefore the map of "differentiation." Note that, because of linear maps' homogeneity and commutativity, we could therefore see the basic rules of differentiation ($(f+g)' = f'+g'$, $(\lambda f)' = \lambda f'$).

**** Integration
Define a $T \in \mathcal{L}(\mathcal{P}(\mathbb{R}), \mathbb{R})$ such that...

\begin{equation}
    Tp = \int^1_0 p(x) dx
\end{equation}

The same reasoning as above show the basic commutativity and homogeneity of integration.

**** $\mathbb{F}^n \to \mathbb{F}^m$
Let $\mathbb{F}^n$ and $\mathbb{F}^m$ be two spaces. Let $m$ and $n$ be positive integers, let $A_{j,k} \in \mathbb{F}$ for $j=1,\ldots,m$ and $k=1,\ldots, n$, and define $T \in \mathcal{L}(\mathbb{F}^n, \mathbb{F}^m)$ by:

\begin{equation}
    T(x_1, \ldots, x_n) = (A_{1,1}x_1+\cdots+A_{1,n}x_n,\ldots,A_{m,1}x_1+\cdots + A_{m,n}x_n)
\end{equation}

Every linear map for $\mathbb{F}^n$ to $\mathbb{F}^m$ look like this.

*** $\mathcal{L}(V,W)$

**** ...is a vector space
Suppose that $S, T \in \mathcal{L}(V,W)$

- $(S+T)(v) = Sv + Tv$
- $(\lambda T)(v) = \lambda(Tv)$

Which, after some wrangling which we will do below, shows that $\mathcal{L} (V,W)$ is a vector space. 

**** ...has the "product" defined
Suppose, now, that $T \in \mathcal{L}(V,W), M \in \mathcal{L}(W,V)$ 

$(TM)(v) = T(Mv)$

$TM$, therefore, is simply $T \circ M$ but when they are linear maps they are considered the "product"

Linear map products are...

- Associative $(T_1 T_2) T_3 = T_1 (T_2 T_3)$
- Identitative $TI = IT = T$, if $I$ is the identity in the domain of $T$
- Distributive $(S_1 + S_2)T = S_1 T S_2 T$, and visa versa

**** Takes $0$ to $0$
Suppose $T$ is a linear map $T: V \to W$, then $T(0) = 0$, as defined in both spaces. So zeros stay zero.

*** Linear Maps & Basis of the Domain
Suppose $v_1, \ldots, v_n$ is a basis of $V$ and $w_1, \ldots, w_n \in W$, then there exists a **unique** linear map $T: V \to W\ s.t.\ Tv_j = w_j$.

That, if a list of vectors is a basis of the domain of linear maps, there exists a unique linear map by which any $n$ (length of the basis of domain) vectors in the codomain could be the output of the linear map applied to each basis.

** In-Text Experiences

*** Linear Map from Basis of Domain is Unique
We first show that there is a linear map $T: V \to W$.

Define:

\begin{equation}
    T(c_1 v_1 + \cdots + c_n v_n) = c_1 w_1 + \cdots + c_n w_n
\end{equation}

We could see that, because $v_1, \ldots, v_n$ is a basis of $V$, the entirety of $V$ is in domain for $T$ (as inputs to $T$ is a linear combination of all elements in $V$). $T$ is furthermore a function as every element in $V$ could be /uniquely/ written (as $v_1, \ldots, v_n$ is a basis) as a linear combination. Therefore, for the expression above, every $Tv_j =$ some $w_j$ by simply setting $c_j=1, j_{not\ j} = 0$.

We now proof the function $T$ is a linear map.

\begin{align}
    T(u+v) &= T(a_1v_1 + \cdots + a_n v_n + c_1v_1 + \cdots + c_nv_n) \\
&= T((a_1+c_1) v_1 + \cdots + (a_n+c_n) v_n) \\
&= (a_1+c_1) w_1 + \cdots + (a_n+c_n) w_n \\
&= a_1w_1 + \cdots + a_n w_n + c_1w_1 + \cdots + c_nw_n \\
&= (a_1w_1 + \cdots + a_n w_n) + (c_1w_1 + \cdots + c_nw_n) \\
&= T(a_1v_1 + \cdots + a_n v_n) + T(c_1v_1 + \cdots + c_nv_n) \\
&= Tu + Tv 
\end{align}

Hence, the function is commutative.

\begin{align}
   T(\lambda v) &= T(\lambda c_1 v_1 + \cdots + \lambda c_n v_n) \\ 
&= \lambda c_1 w_1 + \cdots + \lambda c_n w_n \\
&= \lambda (c_1 w_1 + \cdots + c_n w_n) \\
&= \lambda T(c_1 v_1 + \cdots + c_n v_n) \\
&= \lambda Tv 
\end{align}

The function is therefore homogeneous. Therefore, the function $T$ is a linear map from $V$ to $W$. Any linear map $T$ such that $T v_j = w_j$ would, after each element $v_j$ be scaled by $c_j$ ($T(C-jv_j) = c_jw_j$) (homogeneity) and every $j=1,\ldots,n$ summed ($T(c_1 v_1 + \cdots + c_n v_n) = c_1 w_1 + \cdots + c_n w_n$)  (additivity), we arrive at the unique linear map $T$ as highlighted above.

Hence, $T$ is uniquely determined upon $span(v_1, \ldots, v_n)$. As $v_1,\ldots,v_n$ is the basis of $V$, $T$ is unique upon $V$.

*** Verifying that $\mathcal{L}(V,W)$ is a vector space 
With the operations of scalar multiplication and addition defined, we could check that $\mathcal{L}(V,W)$ is a vector space.

- **Commutativity** $(S+T)(v) = Sv + Tv =(vectors\ are\ commutative) = Tv+Sv = (T+S)(v)$
- **Associativity** per the same token above, as associative property of vectors
- **Additive Identity** $0 \in \mathcal{L}(V,W)$. $0v + Sv = 0 + Sv = Sv$
- **Additive Inverse** Scalar multiplication by $-1$? #TODO
- **Multiplicative Identity** Scalar multiplication by $1$
- **Distribution** $\lambda((S+T)(V)) = \lambda(Sv + Tv) = \lambda Sv + \lambda Tv$

* 3.B

** Things of Note

*** Null Space
For a linear map $T \in \mathcal{L}(V,W)$, the /null space/ of $T$, denoted as $null\ T$, is the subset of $V$ containing vectors which $T$ has the ability to map to the $0$ in $W$. That:

\begin{equation}
    null\ T = \{v\in V: Tv = 0\}
\end{equation}

Axler's Noticings... 

- Notably, if $T = 0$, then the entirety of $V$ is its null space
- Suppose $D \in \mathcal{L}(\mathcal{P}(\mathbb{R}), \mathcal{P}(\mathbb{R}))$ is the differentiation map ($Dp = p'$). Only constants are going to have a derivative of $0$; therefore, the null space of $D$ is the set of constant functions
- Suppose $T \in \mathcal{L}(\mathcal{P}(\mathbb{R}), \mathcal{P}(\mathbb{R}))$ is defined as the $x^2$ by $(Tp)(x) = x^2p(x)$, the only polynomial s.t. $x^2p(x) = 0, \forall x\in \mathbb{R}$ is the $0$; hence $null\ T = \{0\}$.

**** Null Space is a Subspace
Suppose $T \in \mathcal{L}(V,W)$, then $null\ T$ is a subspace of $V$.

*** Injectivity
A function $T: V \to W$ is "injective" if $Tu = Tv$ implies $u=v$. i.e. that each output corresponds to one unique input.

So, if $T$ is "injective", $u\neq v$ implies that $Tu \neq Tv$.

This would imply, of course, that the $null\ T$ of an injective function is simply $\{0\}$ --- because as each output corresponds to a unique input, and $T0 = 0$, so the null space is going to only contain $0$.

*** Range
A function $T$ from $V \to W$ has range defined as the subset of $W$ which takes the form $Tv$, that:

\begin{equation}
    range\ T = \{Tv : v \in V\}
\end{equation}

Based on some reasoning, you could see that the range of a function $T$ is a subspace of $W$.

*** Surjectivity
A function $T: V\to W$ is surjective if its range equals $W$. As in, the entirely of the output space has some input for which it is possible to map to it.

*** **Fundamental Theorem of Linear Maps**
This is kind of important. Because of the above results, we arrive that:

Suppose $V$ is a finite-dimentional vector space and $T \in \mathcal{L}(V, W)$; then, $range\ T$ is finite-dimentional and:

\begin{equation}
    dim\ V = dim\ null\ T + dim\ range\ T
\end{equation}

"the rank-nullity theorem". It is proven below.

*** Map to Smaller Space is not Injective; Map to Larger Space is not Surjective
Suppose $V$ and $W$ are finite-dimentional s.t. $dim\ V > dim\ W$. There are therefore no linear map from $V$ to $W$ which is injective.

Suppose $V$ and $W$ are finite-dimentional s.t. $dim\ V < dim\ W$. There are therefore no linera map from $V$ to $W$ that is surjective.

*** Noticings about Linear Systems
1. A homogenous linear system with more variables than equations has nonzero solutions
2. A inhomogenous linear system with more equations than variables has no solution for some constant terms

** In-Text Exercises

*** Null Space is a Subspace
Suppose $T \in \mathcal{L}(V,W)$. It has a zero (because $T(0) = 0$), and $0+0=0$ and $\lambda0 = 0$, and $0$, as we established a sentence earlier, is in $T$. Therefore, $null\ T$ is a subspace.

*** Proving the Fundamental Theorem of Linear Maps
Suppose $V$ is a finite-dimentional vector space and $T \in \mathcal{L}(V, W)$.

Let $u_1, \ldots, u_m$ be a basis of $null\ T$, then $dim\ null\ T = m$. We know that the null space of $T$ is a subset of $V$, we could extend its basis to that of $V$: that $u_1, \ldots, u_m, v_1, \ldots, v_n$. Hence, $dim\ V = m + n$.

The fact that the list $u_1, \ldots, u_m, v_1, \ldots, v_n$ represents the basis of $V$, we know that every $v \in V$ can be represented as:

\begin{equation}
    v = a_1u_1 + \cdots + a_m u_m + b_1 v_1 + \cdots + b_n v_n,\ where\ a_j,b_j \in \mathbb{F}
\end{equation}

Let's apply $T$ to both ends!

\begin{align}
    Tv &= T(a_1u_1 + \cdots + a_m u_m + b_1 v_1 + \cdots + b_n v_n) \\
&= T(a_1u_1 + \cdots + a_m u_m) + T(b_1 v_1 + \cdots + b_n v_n)\ \text{commutivity} \\
&= 0 + T(b_1 v_1 + \cdots + b_n v_n)\ \text{null space} \\
&= b_1 Tv_1 + \cdots + b_n Tv_n\ \text{commutivity, homogenicity} 
\end{align}

Given this, we now know that $b_1 Tv_1 + \cdots + b_n Tv_n$ is a spanning set of the range of $T$ (as every input to $T$ (i.e. $v \in V$) could be represented as such.) To show that it is now a basis of $range\ T$, we need to show that its linearly independent.

Suppose that there are a set of values $c_1, \ldots, c_n \in \mathbb{F}$ such that...

\begin{equation}
c_1 Tv_1 + \cdots + c_n Tv_n = 0
\end{equation}

Factoring out $T$ using commutivity, we arrive at that:

\begin{equation}
    T(c_1 v_1 + \cdots + c_n v_n) = 0
\end{equation}

Woah! $T$ maps this combination to $0$! Therefore:

\begin{equation}
    c_1 v_1 + \cdots + c_n v_n \in null\ T
\end{equation}

We know that, from the assumptions before, $u_1, \ldots, u_m$ is a basis of $null\ T$. Therefore, we could claim that:

\begin{equation}
    c_1 v_1 + \cdots + c_n v_n  = d_1 u_1 + \cdots + d_m u_m
\end{equation}

We know that, because they were all parts of bases of $V$, $u_1,\ldots, u_m,v_1,\ldots,v_n$ are all linearly independent. There is only one way to write each side as an equation of each other, and $c_1 = \cdots = c_n = d_1 \cdots = d_m = 0$ is one valid way to make this equation valid, therefore --- due to the linear independence --- it is the only way.

Therefore, $c_1 = \cdots = c_n = d_1 \cdots = d_m = 0$. Plugging that allllll the way back to:

\begin{equation}
c_1 Tv_1 + \cdots + c_n Tv_n = 0
\end{equation}

We could now see that $T v_1, \ldots, T v_n$ is linearly independent. Therefore, it is a basis of the range of $T$ (as we established before it spans the range of $T$.)

And lastly! We know that $dim\ V = m + n$. $m$ is the length of the basis of the null space, $n$ is the length of the basis of the range. Therefore $dim\ V = dim\ null\ T + dim\ range\ T\ \blacksquare$. 

Phew.
