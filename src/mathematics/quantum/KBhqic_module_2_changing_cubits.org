:PROPERTIES:
:ID:       51266162-E730-4E82-9D7D-830A5EC4BAE4
:END:
#+title: QIC Module 2: Changing Cubits
#+author: Houjun Liu

* To Do
- Read chapters
- Watch videos

* Core Concepts
- Solving the Shrödinger equation
- We still look at a cubit at a time, but time will now pass: which impacts
  the state of the cubit
- We will learn about evolving a cubit, and gates, etc.

* Introduction

\begin{equation}
   |ud \big> = u \big> \otimes |d\big> = \begin{pmatrix}
1 \\ 0 \end{pmatrix} \otimes \begin{pmatrix}
0 \\ 1 \end{pmatrix} = \begin{pmatrix}
0 \\ 1\\0\\0 \end{pmatrix}
\end{equation}

Instead of thinking about it as /physics/, we will think it about computation.

We propagate /square roots/ of probabilities because time is not real, but is imaginary. The location of something is simply differently likely in various places.

Every /property/ in the quantum work as a cubit.

Change in perspective! Instead of simulating the behavior of cubits with bits, we can simulate them with OTHER CUBITS! So, our tools are now manipulating groups of cubits to simulate other, more complicated groups of cubits.

* Shrödinger's Equations
\begin{equation}
   |\Psi (t) \big> = U(t) | \Psi(0)\big> 
\end{equation}

We will use $\Psi$ to represents a function that dumps a cubit. We will act on it by a unitary matrix function $U$, so $U(t) | \Psi(0) \big>$, represents an evolution. $U$ encode all possible next transition states.

** What's a unitary matrix again?

\begin{equation}
   U^*U = I 
\end{equation}

** What is a Hermitian again?

\begin{equation}
   H = H^* 
\end{equation}

** Basic Properties
\begin{equation}
   \big< \Psi (0) | \Phi (0) \big> = 0  \Rightarrow  \big< \Psi (t) | \Phi (t) \big> = 0
\end{equation}

This claim makes intuitive sense: if we have rigid rotations $U$, perpendicular cubits, however you rotate the pair, you will result in perpendicular cubits.

\begin{equation}
   U(\epsilon) = I-i\epsilon H 
\end{equation}

At every state, in order to maintain unitary in $U$, we have to subtract a small amount of a hermitian matrix. "Think about $H$ as the derivative at zero of $U$"

We can see, of course:

\begin{equation}
   (I+ i \epsilon H^*)(I- i \epsilon H)  = I
\end{equation}

because $\epsilon^2$ is really small, so you get the same thing back.

** The Schrodinger Equation at the Last Bit of Class
\begin{equation}
   \hbar \frac{\partial | \Psi \big>}{\partial t}  = -i H | \Psi \big>
\end{equation}

That the change in $|\Psi\big>$ over time is simply the $-i$ times the hermitian matrix.  And $\hbar$, Planck's constant to get the energy scale right.

And solving it for $|\Psi\big>$, we will result in the state at any point to model the evolution.

Partial differential equations can be treated as a limit of a system of ordinary differential equations.

* Solving Shrödinger's Equations
Let's start with a state:

\begin{equation}
    |A \big> = \sum_i \alpha_i | \lambda_i \big>
\end{equation}

So let's take a linear operator $L$, for whom $\lambda$ is an eigenvector:

\begin{equation}
   L | A \big> =\sum_i \alpha_i \lambda _i | \lambda _i \big> 
\end{equation}

You will notice that, after applying the linear operator, the eigenvector just get scaled by the eigenvalue. (duh)

Lastly, if we get the expected value:

\begin{align}
   \big<L\big> &= \big<A | L | A\big> \\
&= \sum_j {\alpha_j}^* \big<\lambda_j|\sum_i \alpha_i \lambda_i | \lambda i \big>
\end{align}

We know that eigenvectors are orthogonal, so:

\begin{align}
   \big<L\big> &= \big<A | L | A\big> \\
&= \sum_j {\alpha_j}^* \big<\lambda_j|\sum_i \alpha_i \lambda_i | \lambda i \big>\\
&= \sum_j {\alpha_i}^* \alpha _i \lambda_i
\end{align}

Therefore, you just end up with the probably of the sum of the eigenvectors coming up.

Under Schrodinger's model, we don't change the state, we change the operator.

-----

If we want to find the expected value of a change....

\begin{align}
   &\frac{d}{dt} \big <\Psi(t)|L|\Psi(t)\big>\\
\Rightarrow&\frac{d}{dt} ((\big <\Psi(t)|)(L|\Psi(t)\big>))\\
\Rightarrow&\frac{d}{dt} (\big <\Psi(t)|)(L|\Psi(t)\big>) + \frac{d}{dt} (L|\Psi(t)\big>)(\big <\Psi(t)|)
\end{align}

We will get:

#+DOWNLOADED: screenshot @ 2022-03-04 09:28:18
[[file:2022-03-04_09-28-18_screenshot.png]]

back.

We can summarize this expression to say:

\begin{equation}
   \frac{d}{dt} \big<L\big> = \frac{i}{\hbar} \big<[H, L]\big>
\end{equation}

where, the "commutator" $[A,B] = AB-BA$. (There is a cool property, if you think about it, that $[cA,B] = c[A,B]$.

"the change in expected value by some linear operator $L$ is that for the commutator $HL$". For every observable, its rate of change is proportional to the comutator.

Furthermore, we have some:

\begin{equation}
    [Q,H] = 0
\end{equation}

if observation and evolution has no ordering difference, the change in expected value would be zero.

We can actually decompose $|\Psi(t)\big>$ down into the sum of all components multiplied to the eigen 

#+DOWNLOADED: screenshot @ 2022-03-04 09:56:38
[[file:2022-03-04_09-56-38_screenshot.png]]


* Boolean Operators, etc.

** Boolean Inner Product
Boolean strings of length $m$, $x \cdot y$ is their **boolean inner product**, defined to be:

\begin{equation}
x_1 y_1 \oplus \cdots \oplus x_my_m
\end{equation}

here, $\oplus$ means exclusive or (addition mod 2) (1+1 is zero, 0+1 is 1, etc.). Odd number of trues is true, even number of trues is false.

** And now, a boolean network
We can have a boolean network in classical framing:

#+DOWNLOADED: screenshot @ 2022-03-02 09:10:04
[[file:2022-03-02_09-10-04_screenshot.png]]

But now, can you simplify this? Is there a simpler way to represent majority---how does the simplest way of computing the majority scale with the inputs.

Does the number of nodes grow proportionally to the number of inputs? I.e.:

\begin{equation}
   \theta(N) =^? \#\ of\ nodes 
\end{equation}

** Controlled Not
\begin{equation}
   C_{not}(x_1, x_2) = \begin{cases}
x_1\ if\ x_2 = 0\\
1-x_1\ if\ x_2 = 1\\
\end{cases}
\end{equation}

* A Guest Lecture
[[id:CF365519-4623-44E1-A3EA-591A586C74CF][Tim Hosgood]]

* Parallel vs Series
- Tensoring matricies are  parallel operations
- Producting matricies are series operations

